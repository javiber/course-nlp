{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open susbtitles parallel dataset\n",
    "\n",
    "Downloaded the open subtitles dataset from [opus](http://opus.nlpl.eu/OpenSubtitles-v2018.php).\n",
    "More specifically these where the commands run:\n",
    "\n",
    "```bash\n",
    "wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/xml/en.zip\n",
    "wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/xml/es.zip\n",
    "wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/xml/en-es.xml.gz\n",
    "```\n",
    "These files where uncompresed and arranged, they sum up to ~190G:\n",
    "```\n",
    "125G\topen_subtitles/en\n",
    "60G \topen_subtitles/es\n",
    "3.4G\topen_subtitles/en-es.xml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.home()/'open_subtitles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/javiber/open_subtitles/es'),\n",
       " PosixPath('/home/javiber/open_subtitles/README'),\n",
       " PosixPath('/home/javiber/open_subtitles/en'),\n",
       " PosixPath('/home/javiber/open_subtitles/en-es.xml'),\n",
       " PosixPath('/home/javiber/open_subtitles/LICENSE'),\n",
       " PosixPath('/home/javiber/open_subtitles/INFO')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(path.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XCES file\n",
    "\n",
    "The `en-es.xml` file is the one that links what subtitles should be paired together but it doesn't contain the text, that's on the other two files. The file is in XML and could be parsed with `etree` but that would take almost all of the 63B of memory available, leaving little room for processing loading the two subtitle files that we need to load. For that reason a more low level approach will be made where we can control the amount of memory we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path/'en-es.xml') as ces:\n",
    "    # readline will return '' when is the end of the file\n",
    "    lines = ces.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('123', '1 2 3', '1 2', '0.123')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'<link id=\"SL(\\d+)\" xtargets=\"([^;]*);([^\"]*)\" (?:overlap=\"([^\"]+)\" )?/>',\n",
    "    '<link id=\"SL123\" xtargets=\"1 2 3;1 2\" overlap=\"0.123\" />').groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "link_grp_re = re.compile(r'^\\s*<linkGrp .* fromDoc=\"([^\"]+)\" toDoc=\"([^\"]+)\".*>\\s*$')\n",
    "link_re = re.compile(r'<link id=\"SL(\\d+)\" xtargets=\"([^;]*);([^\"]*)\" (?:overlap=\"([^\"]+)\" )?/>')\n",
    "\n",
    "assert link_re.search('<link id=\"SL2\" xtargets=\"3;\" />').groups() == ('2', '3', '', None)\n",
    "assert link_re.search(\n",
    "    '<link id=\"SL123\" xtargets=\"1 2 3;1 2\" overlap=\"0.123\" />').groups() == ('123', '1 2 3', '1 2', '0.123')\n",
    "\n",
    "def link_grp_generator():\n",
    "    # read all lines on the file\n",
    "    with open(path/'en-es.xml') as ces:\n",
    "        lines = ces.readlines()\n",
    "        \n",
    "    ignored_grps = 0\n",
    "    length = len(lines)\n",
    "    i = 0\n",
    "    while i < length:\n",
    "        if lines[i].startswith('<linkGrp'):  # Link group start\n",
    "            # parse link group\n",
    "            m = link_grp_re.search(lines[i])\n",
    "            if m is None:\n",
    "                raise ValueError(f'could not parse link grp line \\'{lines[i]}\\'')\n",
    "            en_file, es_file = m.groups()\n",
    "            \n",
    "            # parse links on this group\n",
    "            links = []\n",
    "            i += 1  # move to next line\n",
    "            while '</linkGrp>' not in lines[i]:\n",
    "                # parse link\n",
    "                m = link_re.search(lines[i])\n",
    "                if m is None:\n",
    "                    raise ValueError(f'could not parse link line \\'{lines[i]}\\'')\n",
    "                link_id, en_ids, es_ids, overlap = m.groups()\n",
    "                # some subtitles don't have a correspondent so we ignore them\n",
    "                if en_ids and es_ids and overlap is not None:\n",
    "                    links.append({\n",
    "                        'id': link_id,\n",
    "                        'en_ids': en_ids, \n",
    "                        'es_ids': es_ids,\n",
    "                        'overlap': float(overlap[0])\n",
    "                    })\n",
    "\n",
    "                i += 1  # move to next line                  \n",
    "\n",
    "            # filter some link groups where the average overlap is not great\n",
    "            # in our experience that seems to indicate that the subtitles don't align very well\n",
    "            # which leads to errors.\n",
    "            if len(links) and sum([x['overlap'] for x in links])/len(links) > 0.7:\n",
    "                yield {\"en_file\": en_file, \"es_file\": es_file, \"links\": links}\n",
    "            else:\n",
    "                ignored_grps += 1\n",
    "\n",
    "        i += 1  # move to next line \n",
    "    print(f\"Ignored {ignored_grps} groups due to low overlap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lg(link_grp):\n",
    "    # ignore '.gz' extension\n",
    "    en_doc = link_grp['en_file'][:-3]\n",
    "    es_doc = link_grp['es_file'][:-3]\n",
    "    \n",
    "    en_root = et.parse(path/en_doc).getroot()\n",
    "    es_root = et.parse(path/es_doc).getroot()\n",
    "    \n",
    "    d = []\n",
    "    for link in link_grp['links']:\n",
    "        # split ids by space. the relation between subtitles is many to many\n",
    "        en_ids = link['en_ids'].split(' ')\n",
    "        es_ids = link['es_ids'].split(' ')\n",
    "        \n",
    "        # get all texts for both languages\n",
    "        en_texts = []\n",
    "        for i in en_ids:\n",
    "            en_texts += [w.text for w in en_root.findall(f'.//s[@id=\"{i}\"]/w')]\n",
    "            \n",
    "        es_texts = []\n",
    "        for i in es_ids:\n",
    "            es_texts += [w.text for w in es_root.findall(f'.//s[@id=\"{i}\"]/w')]\n",
    "            \n",
    "        d.append((\n",
    "            '#'.join([en_doc, es_doc]),  # link group id\n",
    "            link.get('id').replace('SL', ''),  # link id\n",
    "            ' '.join(en_texts),\n",
    "            ' '.join(es_texts)\n",
    "        ))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081d393dbad6489a8b035595e0cee28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=77652), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored 65915 groups due to low overlap\n",
      "wrote 7996831 examples\n",
      "CPU times: user 2min 36s, sys: 7.92 s, total: 2min 44s\n",
      "Wall time: 51min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def process():\n",
    "    raw_file = open('subtitles2.csv', 'w')\n",
    "    csv_file = csv.writer(raw_file)\n",
    "    csv_file.writerow(['files', 'sub_id', 'en', 'es'])  # headers\n",
    "\n",
    "    buffer = []\n",
    "    max_buffer = 10000\n",
    "    total = 0\n",
    "    with Pool(12) as p:\n",
    "        for rs in tqdm(p.imap_unordered(process_lg, link_grp_generator()), total=77652, smoothing=0.1):\n",
    "            buffer += rs\n",
    "            if len(buffer) > max_buffer:\n",
    "                csv_file.writerows(buffer)\n",
    "                total += len(buffer)\n",
    "                buffer = []\n",
    "        if buffer:\n",
    "            total += len(buffer)\n",
    "            csv_file.writerows(rs)\n",
    "\n",
    "    raw_file.close()\n",
    "    print(f'wrote {total} examples')\n",
    "\n",
    "process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiber/.local/share/virtualenvs/javiber-jarMRqt3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7990657"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('subtitles2.csv')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not it cares the university !\n",
      "¡ No importa la universidad !\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "I said , please go away\n",
      "Dije , por favor se vayan .\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Why can 't you clean up my messes ?\n",
      "¿ Por qué no puedes arreglar mi desastre ?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "That was our new proximity alarm .\n",
      "Eso fue nuestro nuevo La alarma de proximidad .\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "What did I do ?\n",
      "¿ Qué he hecho ?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Rachel ?\n",
      "Rachel ?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Yeah .\n",
      "Sí .\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Um , I appreciate you seeing me .\n",
      "Te agradezco que accedieras a verme .\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "With altruistic donors ?\n",
      "¿ Con donantes altruistas ?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "It enhances my strength ... my focus .\n",
      "Mejora mi fuerza ... mi concentración .\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "for _, x in df.sample(10).iterrows():\n",
    "    print(x.en)\n",
    "    print(x.es)\n",
    "    print('~'*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
