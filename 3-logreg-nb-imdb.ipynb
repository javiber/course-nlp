{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "3-logreg-nb-imdb.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW2S3vpFTKaJ",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Classification of Movie Reviews (using Naive Bayes, Logistic Regression, and Ngrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaGf18fbTKaL",
        "colab_type": "text"
      },
      "source": [
        "The purpose of this notebook is to cover Naive Bayes, Logistic regression, and ngrams (some pretty classic techniques!) for sentiment classification.  We will be using sklearn and the fastai library.\n",
        "\n",
        "In a future lesson, we will tackle this same problem of sentiment classification using deep learning, so that you can compare the two approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm7XMa6YTKaM",
        "colab_type": "text"
      },
      "source": [
        "The content here was extended from [Lesson 10 of the fast.ai Machine Learning course](https://course.fast.ai/lessonsml1/lesson10.html). Linear model is pretty close to the state of the art here.  Jeremy surpassed state of the art using a RNN in fall 2017."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "0dTuaxI6TKaN",
        "colab_type": "text"
      },
      "source": [
        "## The fastai library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "m9P7V9kCTKaO",
        "colab_type": "text"
      },
      "source": [
        "We will begin using [the fastai library](https://docs.fast.ai) (version 1.0) in this notebook.  We will use it more once we move on to neural networks.\n",
        "\n",
        "The fastai library is built on top of PyTorch and encodes many state-of-the-art best practices. It is used in production at a number of companies.  You can read more about it here:\n",
        "\n",
        "- [Fast.ai's software could radically democratize AI](https://www.zdnet.com/article/fast-ais-new-software-could-radically-democratize-ai/) (ZDNet)\n",
        "\n",
        "- [fastai v1 for PyTorch: Fast and accurate neural nets using modern best practices](https://www.fast.ai/2018/10/02/fastai-ai/) (fast.ai)\n",
        "\n",
        "- [fastai docs](https://docs.fast.ai/)\n",
        "\n",
        "### Installation\n",
        "\n",
        "With conda:\n",
        "\n",
        "`conda install -c pytorch -c fastai fastai=1.0`\n",
        "\n",
        "Or with pip:\n",
        "\n",
        "`pip install fastai==1.0`\n",
        "\n",
        "More [installation information here](https://github.com/fastai/fastai/blob/master/README.md).\n",
        "\n",
        "Beginning in lesson 4, we will be using GPUs, so if you want, you could switch to a [cloud option](https://course.fast.ai/#using-a-gpu) now to setup fastai."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY2iLtYQUhOb",
        "colab_type": "code",
        "outputId": "21fc8cfa-ddc5-44d2-e3e6-be93f747d9f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        }
      },
      "source": [
        "!pip install --upgrade fastai"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/16/f5e9d5a61e3c911c1ab26ca05d27be86b0d0b44ecd269e76526ba0967aea/fastai-1.0.55-py3-none-any.whl (230kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: typing; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (3.7.4)\n",
            "Requirement already satisfied, skipping upgrade: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.1.6)\n",
            "Requirement already satisfied, skipping upgrade: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai) (2.6.9)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai) (7.352.0)\n",
            "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (4.3.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (0.6)\n",
            "Requirement already satisfied, skipping upgrade: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai) (1.2.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (19.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (0.24.2)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.0.3)\n",
            "Requirement already satisfied, skipping upgrade: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.1.21)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchvision->fastai) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.4)\n",
            "Requirement already satisfied, skipping upgrade: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (7.0.8)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.0.7)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai) (0.46)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2019.6.16)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy>=2.0.18->fastai) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->fastai) (41.0.1)\n",
            "Installing collected packages: fastai\n",
            "  Found existing installation: fastai 1.0.0\n",
            "    Uninstalling fastai-1.0.0:\n",
            "      Successfully uninstalled fastai-1.0.0\n",
            "Successfully installed fastai-1.0.55\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "fastai"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEGjdNh8TKaP",
        "colab_type": "text"
      },
      "source": [
        "## IMDB dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mDbyUzqTKaQ",
        "colab_type": "text"
      },
      "source": [
        "The [large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) contains a collection of 50,000 reviews from IMDB, We will use the version hosted as part [fast.ai datasets](https://course.fast.ai/datasets.html) on AWS Open Datasets. \n",
        "\n",
        "The dataset contains an even number of positive and negative reviews. The authors considered only highly polarized reviews. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. Neutral reviews are not included in the dataset. The dataset is divided into training and test sets. The training set is the same 25,000 labeled reviews.\n",
        "\n",
        "The **sentiment classification task** consists of predicting the polarity (positive or negative) of a given text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOCfJfolTKaR",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UcVb8BDFTKaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-IRBmhvTKaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiHTRzIRTKaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.feature_extraction.text as sklearn_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXFQSlk9TKac",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizing and term document matrix creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqix5HbNTKac",
        "colab_type": "text"
      },
      "source": [
        "fast.ai has a number of [datasets hosted via AWS Open Datasets](https://course.fast.ai/datasets.html) for easy download. We can see them by checking the docs for URLs (remember `??` is a helpful command):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XduUXb_fTKad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "?? URLs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyytlpoWTKag",
        "colab_type": "text"
      },
      "source": [
        "It is always good to start working on a sample of your data before you use the full dataset-- this allows for quicker computations as you debug and get your code working. For IMDB, there is a sample dataset already available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpeaVHA7TKah",
        "colab_type": "code",
        "outputId": "d3ff39e3-7a5e-46c5-99ac-3179b23e2eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "path = untar_data(URLs.IMDB_SAMPLE)\n",
        "path"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.fastai/data/imdb_sample')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdiJZLYsTKam",
        "colab_type": "text"
      },
      "source": [
        "We are not going to use this dataframe, but are just loading it to get a sense of what our data looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6l4EVWSDTKan",
        "colab_type": "code",
        "outputId": "5b7d79af-a344-467b-bf55-31b7b2832282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv(path/'texts.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie wi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most u...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               text  is_valid\n",
              "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
              "1  positive  This is a extremely well-made film. The acting...     False\n",
              "2  negative  Every once in a long while a movie will come a...     False\n",
              "3  positive  Name just says it all. I watched this movie wi...     False\n",
              "4  negative  This movie succeeds at being one of the most u...     False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkdMt9fyTKat",
        "colab_type": "text"
      },
      "source": [
        "We will be using [TextList](https://docs.fast.ai/text.data.html#TextList) from the fastai library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvO_Zc9-TKau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_small = (TextList.from_csv(path, 'texts.csv', cols='text')\n",
        "                         .split_from_df(col=2)\n",
        "                         .label_from_df(cols=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLZh_yNeTKax",
        "colab_type": "text"
      },
      "source": [
        "### Exploring what our data looks like"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt1uSWskTKax",
        "colab_type": "text"
      },
      "source": [
        "A good first step for any data problem is to explore the data and get a sense of what it looks like.  In this case we are looking at movie reviews, which have been labeled as \"positive\" or \"negative\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3u4c_sQTKay",
        "colab_type": "code",
        "outputId": "99a31c1f-62f0-4ac6-ae17-abcaaa498eb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "reviews_small.valid.x[0], reviews_small.valid.y[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos xxmaj this very funny xxmaj british comedy shows what might happen if a section of xxmaj london , in this case xxmaj xxunk , were to xxunk itself independent from the rest of the xxup uk and its laws , xxunk & post - war xxunk . xxmaj merry xxunk is what would happen . \n",
              "  \n",
              "   xxmaj the explosion of a wartime bomb leads to the xxunk of ancient xxunk which show that xxmaj xxunk was xxunk to the xxmaj xxunk of xxmaj xxunk xxunk ago , a small historical xxunk long since forgotten . xxmaj to the new xxmaj xxunk , however , this is an unexpected opportunity to live as they please , free from any xxunk from xxmaj xxunk . \n",
              "  \n",
              "   xxmaj stanley xxmaj xxunk is excellent as the minor city xxunk who suddenly finds himself leading one of the world 's xxunk xxunk . xxmaj xxunk xxmaj margaret xxmaj xxunk is a delight as the history professor who sides with xxmaj xxunk . xxmaj others in the stand - out cast include xxmaj xxunk xxmaj xxunk , xxmaj paul xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj xxunk & xxmaj sir xxmaj michael xxmaj xxunk . \n",
              "  \n",
              "   xxmaj welcome to xxmaj xxunk !, Category positive)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyNI4bunTKa2",
        "colab_type": "text"
      },
      "source": [
        "In NLP, a **token** is the basic unit of processing (what the tokens are depends on the application and your choices). Here, the tokens mostly correspond to words or punctuation, as well as several special tokens, corresponding to unknown words, capitalization, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkS9oq4iTKa2",
        "colab_type": "text"
      },
      "source": [
        "All those tokens starting with \"xx\" are fastai special tokens.  You can see the list of all of them and their meanings ([in the fastai docs](https://docs.fast.ai/text.transform.html)): \n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-psHWnkbNDz",
        "colab_type": "code",
        "outputId": "6e344db5-b5e3-4886-8271-17df36ac8657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tok = SpacyTokenizer('en')\n",
        "tokenizer.process_text(\"Hello world!!! I'm learning NLP\", tok)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xxmaj',\n",
              " 'hello',\n",
              " 'world',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " 'i',\n",
              " \"'m\",\n",
              " 'learning',\n",
              " 'xxup',\n",
              " 'nlp']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs97Hgz2TKa3",
        "colab_type": "code",
        "outputId": "3dfbc6f8-b92e-45b0-c3b8-97ae3aa46db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(reviews_small.train.x), len(reviews_small.valid.x)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X52RAz4TKa7",
        "colab_type": "text"
      },
      "source": [
        "Notice that ints-to-string and string-to-ints have different lengths.  Think for a moment about why this is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yppsrbejTKa8",
        "colab_type": "code",
        "outputId": "01832556-e62f-488c-a28c-ac533582e600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(reviews_small.vocab.itos), len(reviews_small.vocab.stoi)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6008, 19161)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMOGszL-TKbA",
        "colab_type": "code",
        "outputId": "df6921e0-a2a4-4d80-c2d6-e7e08f1cb6ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "reviews_small.vocab.stoi['language']"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "917"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3WKNyRfTKbD",
        "colab_type": "code",
        "outputId": "db5109c0-593f-4f6d-94a4-07d94702e4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "reviews_small.vocab.itos[917]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'language'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vEeNFxp3TKbI",
        "colab_type": "code",
        "outputId": "ddf3fd77-b3cb-4839-dd13-eefb7390239c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "source": [
        "reviews_small.vocab.itos[0:30], reviews_small.vocab.itos[-10:]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['xxunk',\n",
              "  'xxpad',\n",
              "  'xxbos',\n",
              "  'xxeos',\n",
              "  'xxfld',\n",
              "  'xxmaj',\n",
              "  'xxup',\n",
              "  'xxrep',\n",
              "  'xxwrep',\n",
              "  'the',\n",
              "  '.',\n",
              "  ',',\n",
              "  'and',\n",
              "  'a',\n",
              "  'of',\n",
              "  'to',\n",
              "  'is',\n",
              "  'it',\n",
              "  'in',\n",
              "  'i',\n",
              "  'that',\n",
              "  'this',\n",
              "  '\"',\n",
              "  \"'s\",\n",
              "  '\\n \\n ',\n",
              "  '-',\n",
              "  'was',\n",
              "  'as',\n",
              "  'for',\n",
              "  'movie'],\n",
              " ['flik',\n",
              "  'ladder',\n",
              "  'wtc',\n",
              "  'portuguese',\n",
              "  'della',\n",
              "  'contractor',\n",
              "  'coaxes',\n",
              "  'mabuse',\n",
              "  'greyson',\n",
              "  'sollett'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia4ec1b1TKbR",
        "colab_type": "code",
        "outputId": "512c1766-89ac-4a06-cf91-019a87e80e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "reviews_small.vocab.itos[:20]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xxunk',\n",
              " 'xxpad',\n",
              " 'xxbos',\n",
              " 'xxeos',\n",
              " 'xxfld',\n",
              " 'xxmaj',\n",
              " 'xxup',\n",
              " 'xxrep',\n",
              " 'xxwrep',\n",
              " 'the',\n",
              " '.',\n",
              " ',',\n",
              " 'and',\n",
              " 'a',\n",
              " 'of',\n",
              " 'to',\n",
              " 'is',\n",
              " 'it',\n",
              " 'in',\n",
              " 'i']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QCkleZpGTKbU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d00580e-d4b1-4390-d05b-4434842ae403"
      },
      "source": [
        "reviews_small.vocab.stoi"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'xxunk': 0,\n",
              "             'xxpad': 1,\n",
              "             'xxbos': 2,\n",
              "             'xxeos': 3,\n",
              "             'xxfld': 4,\n",
              "             'xxmaj': 5,\n",
              "             'xxup': 6,\n",
              "             'xxrep': 7,\n",
              "             'xxwrep': 8,\n",
              "             'the': 9,\n",
              "             '.': 10,\n",
              "             ',': 11,\n",
              "             'and': 12,\n",
              "             'a': 13,\n",
              "             'of': 14,\n",
              "             'to': 15,\n",
              "             'is': 16,\n",
              "             'it': 17,\n",
              "             'in': 18,\n",
              "             'i': 19,\n",
              "             'that': 20,\n",
              "             'this': 21,\n",
              "             '\"': 22,\n",
              "             \"'s\": 23,\n",
              "             '\\n \\n ': 24,\n",
              "             '-': 25,\n",
              "             'was': 26,\n",
              "             'as': 27,\n",
              "             'for': 28,\n",
              "             'movie': 29,\n",
              "             'with': 30,\n",
              "             'but': 31,\n",
              "             'film': 32,\n",
              "             'you': 33,\n",
              "             ')': 34,\n",
              "             'on': 35,\n",
              "             '(': 36,\n",
              "             \"n't\": 37,\n",
              "             'are': 38,\n",
              "             'he': 39,\n",
              "             'his': 40,\n",
              "             'not': 41,\n",
              "             'have': 42,\n",
              "             'be': 43,\n",
              "             'one': 44,\n",
              "             'they': 45,\n",
              "             'all': 46,\n",
              "             'at': 47,\n",
              "             'by': 48,\n",
              "             'an': 49,\n",
              "             'from': 50,\n",
              "             'like': 51,\n",
              "             '!': 52,\n",
              "             'so': 53,\n",
              "             'who': 54,\n",
              "             'there': 55,\n",
              "             'about': 56,\n",
              "             'just': 57,\n",
              "             'out': 58,\n",
              "             'if': 59,\n",
              "             'or': 60,\n",
              "             'do': 61,\n",
              "             \"'\": 62,\n",
              "             'what': 63,\n",
              "             'her': 64,\n",
              "             'has': 65,\n",
              "             'some': 66,\n",
              "             'more': 67,\n",
              "             'good': 68,\n",
              "             'when': 69,\n",
              "             'up': 70,\n",
              "             'very': 71,\n",
              "             '?': 72,\n",
              "             'she': 73,\n",
              "             'would': 74,\n",
              "             'no': 75,\n",
              "             'really': 76,\n",
              "             'were': 77,\n",
              "             'their': 78,\n",
              "             'my': 79,\n",
              "             'had': 80,\n",
              "             'time': 81,\n",
              "             'can': 82,\n",
              "             'only': 83,\n",
              "             'which': 84,\n",
              "             'even': 85,\n",
              "             'see': 86,\n",
              "             'story': 87,\n",
              "             'me': 88,\n",
              "             'into': 89,\n",
              "             'did': 90,\n",
              "             ':': 91,\n",
              "             'well': 92,\n",
              "             'we': 93,\n",
              "             'will': 94,\n",
              "             'does': 95,\n",
              "             'than': 96,\n",
              "             'also': 97,\n",
              "             'get': 98,\n",
              "             '...': 99,\n",
              "             'people': 100,\n",
              "             'other': 101,\n",
              "             'bad': 102,\n",
              "             'been': 103,\n",
              "             'could': 104,\n",
              "             'first': 105,\n",
              "             'much': 106,\n",
              "             'how': 107,\n",
              "             'most': 108,\n",
              "             'any': 109,\n",
              "             'because': 110,\n",
              "             'two': 111,\n",
              "             'then': 112,\n",
              "             'great': 113,\n",
              "             'him': 114,\n",
              "             'its': 115,\n",
              "             'too': 116,\n",
              "             'made': 117,\n",
              "             'them': 118,\n",
              "             'after': 119,\n",
              "             'movies': 120,\n",
              "             'make': 121,\n",
              "             '/': 122,\n",
              "             'way': 123,\n",
              "             'think': 124,\n",
              "             'never': 125,\n",
              "             'watch': 126,\n",
              "             'acting': 127,\n",
              "             'seen': 128,\n",
              "             ';': 129,\n",
              "             'films': 130,\n",
              "             'plot': 131,\n",
              "             'being': 132,\n",
              "             'many': 133,\n",
              "             'over': 134,\n",
              "             'where': 135,\n",
              "             'character': 136,\n",
              "             'man': 137,\n",
              "             'little': 138,\n",
              "             'better': 139,\n",
              "             'life': 140,\n",
              "             'characters': 141,\n",
              "             'love': 142,\n",
              "             'your': 143,\n",
              "             'here': 144,\n",
              "             'know': 145,\n",
              "             'scenes': 146,\n",
              "             'best': 147,\n",
              "             'end': 148,\n",
              "             'show': 149,\n",
              "             'while': 150,\n",
              "             'through': 151,\n",
              "             'should': 152,\n",
              "             'off': 153,\n",
              "             'ever': 154,\n",
              "             'these': 155,\n",
              "             'go': 156,\n",
              "             'such': 157,\n",
              "             'say': 158,\n",
              "             '--': 159,\n",
              "             'something': 160,\n",
              "             'scene': 161,\n",
              "             'still': 162,\n",
              "             'before': 163,\n",
              "             'though': 164,\n",
              "             'watching': 165,\n",
              "             'between': 166,\n",
              "             'actually': 167,\n",
              "             'old': 168,\n",
              "             '10': 169,\n",
              "             'find': 170,\n",
              "             'back': 171,\n",
              "             'now': 172,\n",
              "             'why': 173,\n",
              "             'years': 174,\n",
              "             \"'ve\": 175,\n",
              "             'actors': 176,\n",
              "             'fact': 177,\n",
              "             'those': 178,\n",
              "             \"'m\": 179,\n",
              "             'thing': 180,\n",
              "             'pretty': 181,\n",
              "             'quite': 182,\n",
              "             'part': 183,\n",
              "             'going': 184,\n",
              "             'same': 185,\n",
              "             'real': 186,\n",
              "             'another': 187,\n",
              "             'down': 188,\n",
              "             'funny': 189,\n",
              "             'nothing': 190,\n",
              "             'look': 191,\n",
              "             'makes': 192,\n",
              "             '*': 193,\n",
              "             'new': 194,\n",
              "             'want': 195,\n",
              "             'action': 196,\n",
              "             '&': 197,\n",
              "             'director': 198,\n",
              "             'work': 199,\n",
              "             'few': 200,\n",
              "             \"'re\": 201,\n",
              "             'seems': 202,\n",
              "             'around': 203,\n",
              "             'world': 204,\n",
              "             'point': 205,\n",
              "             'without': 206,\n",
              "             'cast': 207,\n",
              "             'again': 208,\n",
              "             'own': 209,\n",
              "             'both': 210,\n",
              "             'lot': 211,\n",
              "             'enough': 212,\n",
              "             'every': 213,\n",
              "             'family': 214,\n",
              "             'got': 215,\n",
              "             'ca': 216,\n",
              "             \"'ll\": 217,\n",
              "             'probably': 218,\n",
              "             'big': 219,\n",
              "             'bit': 220,\n",
              "             'might': 221,\n",
              "             'things': 222,\n",
              "             'horror': 223,\n",
              "             'us': 224,\n",
              "             'almost': 225,\n",
              "             'may': 226,\n",
              "             'right': 227,\n",
              "             'must': 228,\n",
              "             'away': 229,\n",
              "             'thought': 230,\n",
              "             'interesting': 231,\n",
              "             'least': 232,\n",
              "             'whole': 233,\n",
              "             'series': 234,\n",
              "             'gets': 235,\n",
              "             'each': 236,\n",
              "             'give': 237,\n",
              "             'young': 238,\n",
              "             'however': 239,\n",
              "             'making': 240,\n",
              "             'day': 241,\n",
              "             'fun': 242,\n",
              "             'anything': 243,\n",
              "             'minutes': 244,\n",
              "             'kind': 245,\n",
              "             'come': 246,\n",
              "             'girl': 247,\n",
              "             'saw': 248,\n",
              "             'script': 249,\n",
              "             'take': 250,\n",
              "             'long': 251,\n",
              "             'times': 252,\n",
              "             'someone': 253,\n",
              "             'found': 254,\n",
              "             'done': 255,\n",
              "             'feel': 256,\n",
              "             'far': 257,\n",
              "             'since': 258,\n",
              "             'role': 259,\n",
              "             'original': 260,\n",
              "             'course': 261,\n",
              "             'goes': 262,\n",
              "             'last': 263,\n",
              "             'true': 264,\n",
              "             'simply': 265,\n",
              "             'always': 266,\n",
              "             \"'d\": 267,\n",
              "             'tv': 268,\n",
              "             'hard': 269,\n",
              "             'place': 270,\n",
              "             'set': 271,\n",
              "             'trying': 272,\n",
              "             'believe': 273,\n",
              "             'shot': 274,\n",
              "             'comes': 275,\n",
              "             'actor': 276,\n",
              "             'yet': 277,\n",
              "             '4': 278,\n",
              "             'having': 279,\n",
              "             'book': 280,\n",
              "             'looks': 281,\n",
              "             'guy': 282,\n",
              "             'screen': 283,\n",
              "             'later': 284,\n",
              "             'shows': 285,\n",
              "             'performance': 286,\n",
              "             'worth': 287,\n",
              "             'comedy': 288,\n",
              "             'sure': 289,\n",
              "             'looking': 290,\n",
              "             'sense': 291,\n",
              "             'star': 292,\n",
              "             'effects': 293,\n",
              "             'read': 294,\n",
              "             'takes': 295,\n",
              "             'although': 296,\n",
              "             'audience': 297,\n",
              "             'ending': 298,\n",
              "             'john': 299,\n",
              "             'anyone': 300,\n",
              "             'worst': 301,\n",
              "             'american': 302,\n",
              "             'year': 303,\n",
              "             'especially': 304,\n",
              "             'women': 305,\n",
              "             'together': 306,\n",
              "             'dvd': 307,\n",
              "             'instead': 308,\n",
              "             'different': 309,\n",
              "             'am': 310,\n",
              "             'woman': 311,\n",
              "             'men': 312,\n",
              "             '2': 313,\n",
              "             'our': 314,\n",
              "             'played': 315,\n",
              "             'music': 316,\n",
              "             'special': 317,\n",
              "             'three': 318,\n",
              "             'rest': 319,\n",
              "             'put': 320,\n",
              "             'maybe': 321,\n",
              "             'wife': 322,\n",
              "             'kids': 323,\n",
              "             'war': 324,\n",
              "             'left': 325,\n",
              "             'black': 326,\n",
              "             'once': 327,\n",
              "             'second': 328,\n",
              "             'watched': 329,\n",
              "             'next': 330,\n",
              "             'friends': 331,\n",
              "             'rather': 332,\n",
              "             'let': 333,\n",
              "             '\\x96': 334,\n",
              "             'job': 335,\n",
              "             'start': 336,\n",
              "             'others': 337,\n",
              "             'budget': 338,\n",
              "             'need': 339,\n",
              "             'mind': 340,\n",
              "             'said': 341,\n",
              "             'main': 342,\n",
              "             'else': 343,\n",
              "             'wrong': 344,\n",
              "             'beautiful': 345,\n",
              "             'half': 346,\n",
              "             'high': 347,\n",
              "             'idea': 348,\n",
              "             'death': 349,\n",
              "             'tell': 350,\n",
              "             'help': 351,\n",
              "             'nice': 352,\n",
              "             'seem': 353,\n",
              "             'perhaps': 354,\n",
              "             'hollywood': 355,\n",
              "             'everyone': 356,\n",
              "             'play': 357,\n",
              "             'case': 358,\n",
              "             'production': 359,\n",
              "             'piece': 360,\n",
              "             'episode': 361,\n",
              "             'camera': 362,\n",
              "             'low': 363,\n",
              "             'already': 364,\n",
              "             'top': 365,\n",
              "             'poor': 366,\n",
              "             'during': 367,\n",
              "             '3': 368,\n",
              "             'stars': 369,\n",
              "             'house': 370,\n",
              "             '..': 371,\n",
              "             'couple': 372,\n",
              "             'boring': 373,\n",
              "             'reason': 374,\n",
              "             'try': 375,\n",
              "             'along': 376,\n",
              "             'name': 377,\n",
              "             'small': 378,\n",
              "             'plays': 379,\n",
              "             'father': 380,\n",
              "             'everything': 381,\n",
              "             'used': 382,\n",
              "             'video': 383,\n",
              "             'getting': 384,\n",
              "             'money': 385,\n",
              "             'full': 386,\n",
              "             'less': 387,\n",
              "             'performances': 388,\n",
              "             'often': 389,\n",
              "             'liked': 390,\n",
              "             'came': 391,\n",
              "             '1': 392,\n",
              "             'robert': 393,\n",
              "             'either': 394,\n",
              "             'fan': 395,\n",
              "             'given': 396,\n",
              "             'hand': 397,\n",
              "             'kill': 398,\n",
              "             'felt': 399,\n",
              "             'yes': 400,\n",
              "             'completely': 401,\n",
              "             'night': 402,\n",
              "             'children': 403,\n",
              "             'himself': 404,\n",
              "             'girls': 405,\n",
              "             'early': 406,\n",
              "             'awful': 407,\n",
              "             'oh': 408,\n",
              "             'live': 409,\n",
              "             'picture': 410,\n",
              "             'parts': 411,\n",
              "             'throughout': 412,\n",
              "             'until': 413,\n",
              "             'become': 414,\n",
              "             'town': 415,\n",
              "             'written': 416,\n",
              "             'terrible': 417,\n",
              "             'turn': 418,\n",
              "             'child': 419,\n",
              "             'despite': 420,\n",
              "             'moments': 421,\n",
              "             'boy': 422,\n",
              "             'problem': 423,\n",
              "             'head': 424,\n",
              "             'stupid': 425,\n",
              "             'beginning': 426,\n",
              "             'home': 427,\n",
              "             'version': 428,\n",
              "             'able': 429,\n",
              "             'excellent': 430,\n",
              "             'sometimes': 431,\n",
              "             'overall': 432,\n",
              "             'recommend': 433,\n",
              "             'sex': 434,\n",
              "             'keep': 435,\n",
              "             'human': 436,\n",
              "             'drama': 437,\n",
              "             'hero': 438,\n",
              "             'supposed': 439,\n",
              "             'seemed': 440,\n",
              "             'use': 441,\n",
              "             'writing': 442,\n",
              "             'wo': 443,\n",
              "             'remember': 444,\n",
              "             'went': 445,\n",
              "             'enjoy': 446,\n",
              "             'classic': 447,\n",
              "             'person': 448,\n",
              "             'killer': 449,\n",
              "             'lost': 450,\n",
              "             'late': 451,\n",
              "             '5': 452,\n",
              "             'title': 453,\n",
              "             'king': 454,\n",
              "             'entire': 455,\n",
              "             'history': 456,\n",
              "             'son': 457,\n",
              "             'school': 458,\n",
              "             'lead': 459,\n",
              "             'english': 460,\n",
              "             'sound': 461,\n",
              "             'cinema': 462,\n",
              "             'seeing': 463,\n",
              "             'unfortunately': 464,\n",
              "             'genre': 465,\n",
              "             'sort': 466,\n",
              "             'mean': 467,\n",
              "             'friend': 468,\n",
              "             'fans': 469,\n",
              "             'close': 470,\n",
              "             'quality': 471,\n",
              "             'definitely': 472,\n",
              "             'james': 473,\n",
              "             'worse': 474,\n",
              "             'says': 475,\n",
              "             'except': 476,\n",
              "             'doing': 477,\n",
              "             'itself': 478,\n",
              "             'past': 479,\n",
              "             'certainly': 480,\n",
              "             'days': 481,\n",
              "             'five': 482,\n",
              "             'dialogue': 483,\n",
              "             'line': 484,\n",
              "             'anyway': 485,\n",
              "             'under': 486,\n",
              "             'tries': 487,\n",
              "             'called': 488,\n",
              "             'fine': 489,\n",
              "             'guys': 490,\n",
              "             'care': 491,\n",
              "             'style': 492,\n",
              "             'hope': 493,\n",
              "             'short': 494,\n",
              "             'lines': 495,\n",
              "             'told': 496,\n",
              "             'car': 497,\n",
              "             'decent': 498,\n",
              "             'brother': 499,\n",
              "             'killed': 500,\n",
              "             'wanted': 501,\n",
              "             'entertaining': 502,\n",
              "             'based': 503,\n",
              "             'absolutely': 504,\n",
              "             'feeling': 505,\n",
              "             'truly': 506,\n",
              "             'etc': 507,\n",
              "             'heard': 508,\n",
              "             'serious': 509,\n",
              "             'run': 510,\n",
              "             'wonderful': 511,\n",
              "             'lives': 512,\n",
              "             'gives': 513,\n",
              "             'moment': 514,\n",
              "             'game': 515,\n",
              "             'documentary': 516,\n",
              "             'self': 517,\n",
              "             'several': 518,\n",
              "             'waste': 519,\n",
              "             'dead': 520,\n",
              "             'blood': 521,\n",
              "             'matter': 522,\n",
              "             'wonder': 523,\n",
              "             'humor': 524,\n",
              "             'thinking': 525,\n",
              "             'against': 526,\n",
              "             'white': 527,\n",
              "             'side': 528,\n",
              "             'works': 529,\n",
              "             'mother': 530,\n",
              "             'flick': 531,\n",
              "             'stuff': 532,\n",
              "             'turns': 533,\n",
              "             'finally': 534,\n",
              "             'loved': 535,\n",
              "             'group': 536,\n",
              "             'wants': 537,\n",
              "             'face': 538,\n",
              "             'guess': 539,\n",
              "             'dark': 540,\n",
              "             'city': 541,\n",
              "             'events': 542,\n",
              "             'starts': 543,\n",
              "             'hour': 544,\n",
              "             'took': 545,\n",
              "             'george': 546,\n",
              "             'themselves': 547,\n",
              "             'red': 548,\n",
              "             'behind': 549,\n",
              "             'talking': 550,\n",
              "             'hit': 551,\n",
              "             'eyes': 552,\n",
              "             'attempt': 553,\n",
              "             'direction': 554,\n",
              "             'novel': 555,\n",
              "             'saying': 556,\n",
              "             'word': 557,\n",
              "             'dull': 558,\n",
              "             'light': 559,\n",
              "             'view': 560,\n",
              "             'playing': 561,\n",
              "             'opinion': 562,\n",
              "             'expect': 563,\n",
              "             'evil': 564,\n",
              "             'ten': 565,\n",
              "             'violence': 566,\n",
              "             'local': 567,\n",
              "             'final': 568,\n",
              "             'gave': 569,\n",
              "             'leave': 570,\n",
              "             'paul': 571,\n",
              "             'crap': 572,\n",
              "             'happens': 573,\n",
              "             'knows': 574,\n",
              "             'problems': 575,\n",
              "             'example': 576,\n",
              "             'relationship': 577,\n",
              "             'non': 578,\n",
              "             'michael': 579,\n",
              "             'victor': 580,\n",
              "             'ridiculous': 581,\n",
              "             'god': 582,\n",
              "             'similar': 583,\n",
              "             'general': 584,\n",
              "             'major': 585,\n",
              "             'bunch': 586,\n",
              "             'sister': 587,\n",
              "             'oscar': 588,\n",
              "             'turned': 589,\n",
              "             'brilliant': 590,\n",
              "             'highly': 591,\n",
              "             'nearly': 592,\n",
              "             'de': 593,\n",
              "             'please': 594,\n",
              "             'romance': 595,\n",
              "             'body': 596,\n",
              "             'extremely': 597,\n",
              "             'mr.': 598,\n",
              "             'soon': 599,\n",
              "             'yourself': 600,\n",
              "             'known': 601,\n",
              "             'lack': 602,\n",
              "             'age': 603,\n",
              "             'interest': 604,\n",
              "             'ago': 605,\n",
              "             'stories': 606,\n",
              "             'exactly': 607,\n",
              "             'finds': 608,\n",
              "             'modern': 609,\n",
              "             'voice': 610,\n",
              "             'perfect': 611,\n",
              "             'heart': 612,\n",
              "             'alone': 613,\n",
              "             'tells': 614,\n",
              "             'daughter': 615,\n",
              "             'directed': 616,\n",
              "             'needs': 617,\n",
              "             'kid': 618,\n",
              "             'lady': 619,\n",
              "             'sad': 620,\n",
              "             'fight': 621,\n",
              "             'happened': 622,\n",
              "             'eye': 623,\n",
              "             'favorite': 624,\n",
              "             'using': 625,\n",
              "             'upon': 626,\n",
              "             'ben': 627,\n",
              "             'none': 628,\n",
              "             'beyond': 629,\n",
              "             'nature': 630,\n",
              "             'change': 631,\n",
              "             'save': 632,\n",
              "             'shots': 633,\n",
              "             'country': 634,\n",
              "             'number': 635,\n",
              "             'shown': 636,\n",
              "             'surprised': 637,\n",
              "             'romantic': 638,\n",
              "             'huge': 639,\n",
              "             'murder': 640,\n",
              "             'steve': 641,\n",
              "             'slow': 642,\n",
              "             'myself': 643,\n",
              "             'woods': 644,\n",
              "             'apparently': 645,\n",
              "             'lake': 646,\n",
              "             'cheap': 647,\n",
              "             'involved': 648,\n",
              "             'roles': 649,\n",
              "             '6': 650,\n",
              "             'gore': 651,\n",
              "             'obviously': 652,\n",
              "             'knew': 653,\n",
              "             'level': 654,\n",
              "             '8': 655,\n",
              "             'experience': 656,\n",
              "             'became': 657,\n",
              "             'gone': 658,\n",
              "             'cover': 659,\n",
              "             'amazing': 660,\n",
              "             'create': 661,\n",
              "             'living': 662,\n",
              "             'usually': 663,\n",
              "             'order': 664,\n",
              "             'monster': 665,\n",
              "             'happen': 666,\n",
              "             'list': 667,\n",
              "             'clearly': 668,\n",
              "             'power': 669,\n",
              "             'features': 670,\n",
              "             're': 671,\n",
              "             'subject': 672,\n",
              "             'across': 673,\n",
              "             'parents': 674,\n",
              "             'seriously': 675,\n",
              "             'ways': 676,\n",
              "             'room': 677,\n",
              "             'filmed': 678,\n",
              "             'cheesy': 679,\n",
              "             'disappointed': 680,\n",
              "             'important': 681,\n",
              "             'plenty': 682,\n",
              "             '7': 683,\n",
              "             'particular': 684,\n",
              "             'started': 685,\n",
              "             'today': 686,\n",
              "             'enjoyed': 687,\n",
              "             'cinematography': 688,\n",
              "             'annoying': 689,\n",
              "             'looked': 690,\n",
              "             'supporting': 691,\n",
              "             'mostly': 692,\n",
              "             'message': 693,\n",
              "             'somewhat': 694,\n",
              "             'viewer': 695,\n",
              "             'type': 696,\n",
              "             'certain': 697,\n",
              "             'release': 698,\n",
              "             'effort': 699,\n",
              "             'possible': 700,\n",
              "             'add': 701,\n",
              "             'figure': 702,\n",
              "             'named': 703,\n",
              "             'wish': 704,\n",
              "             'difficult': 705,\n",
              "             'falls': 706,\n",
              "             'four': 707,\n",
              "             'husband': 708,\n",
              "             'score': 709,\n",
              "             'leads': 710,\n",
              "             'form': 711,\n",
              "             'working': 712,\n",
              "             'writer': 713,\n",
              "             'sets': 714,\n",
              "             'including': 715,\n",
              "             'enjoyable': 716,\n",
              "             'ok': 717,\n",
              "             'note': 718,\n",
              "             'spent': 719,\n",
              "             'review': 720,\n",
              "             'art': 721,\n",
              "             'police': 722,\n",
              "             'sit': 723,\n",
              "             'horrible': 724,\n",
              "             'actress': 725,\n",
              "             'ones': 726,\n",
              "             'bring': 727,\n",
              "             'greatest': 728,\n",
              "             'dance': 729,\n",
              "             'earth': 730,\n",
              "             'becomes': 731,\n",
              "             'happy': 732,\n",
              "             'cut': 733,\n",
              "             'straight': 734,\n",
              "             'soundtrack': 735,\n",
              "             'leading': 736,\n",
              "             'laugh': 737,\n",
              "             'strange': 738,\n",
              "             'space': 739,\n",
              "             'b': 740,\n",
              "             'tale': 741,\n",
              "             'comic': 742,\n",
              "             'near': 743,\n",
              "             'due': 744,\n",
              "             'weak': 745,\n",
              "             'earlier': 746,\n",
              "             'follow': 747,\n",
              "             'british': 748,\n",
              "             'ends': 749,\n",
              "             'typical': 750,\n",
              "             'attention': 751,\n",
              "             'points': 752,\n",
              "             'talent': 753,\n",
              "             'tom': 754,\n",
              "             'female': 755,\n",
              "             'future': 756,\n",
              "             'fall': 757,\n",
              "             'laughs': 758,\n",
              "             'stop': 759,\n",
              "             'easy': 760,\n",
              "             'moving': 761,\n",
              "             'apart': 762,\n",
              "             'chance': 763,\n",
              "             'running': 764,\n",
              "             'york': 765,\n",
              "             'particularly': 766,\n",
              "             'luke': 767,\n",
              "             'bill': 768,\n",
              "             'forced': 769,\n",
              "             'theme': 770,\n",
              "             'rating': 771,\n",
              "             'coming': 772,\n",
              "             'davis': 773,\n",
              "             'totally': 774,\n",
              "             'realistic': 775,\n",
              "             'simple': 776,\n",
              "             'hours': 777,\n",
              "             'taken': 778,\n",
              "             'indeed': 779,\n",
              "             'released': 780,\n",
              "             'sexual': 781,\n",
              "             'feels': 782,\n",
              "             'french': 783,\n",
              "             'screenplay': 784,\n",
              "             'la': 785,\n",
              "             'jokes': 786,\n",
              "             'sequences': 787,\n",
              "             'chase': 788,\n",
              "             'portrayed': 789,\n",
              "             'dramatic': 790,\n",
              "             'mention': 791,\n",
              "             'talk': 792,\n",
              "             'gun': 793,\n",
              "             'thriller': 794,\n",
              "             'jimmy': 795,\n",
              "             'career': 796,\n",
              "             'reality': 797,\n",
              "             'incredibly': 798,\n",
              "             'whether': 799,\n",
              "             'towards': 800,\n",
              "             'easily': 801,\n",
              "             'entertainment': 802,\n",
              "             'feature': 803,\n",
              "             'western': 804,\n",
              "             'dialog': 805,\n",
              "             'business': 806,\n",
              "             'suspense': 807,\n",
              "             'focus': 808,\n",
              "             'doubt': 809,\n",
              "             'possibly': 810,\n",
              "             'water': 811,\n",
              "             'gay': 812,\n",
              "             'blob': 813,\n",
              "             'comments': 814,\n",
              "             'brothers': 815,\n",
              "             'clear': 816,\n",
              "             'agree': 817,\n",
              "             'allen': 818,\n",
              "             'door': 819,\n",
              "             'editing': 820,\n",
              "             'third': 821,\n",
              "             'deserves': 822,\n",
              "             'silly': 823,\n",
              "             'fantastic': 824,\n",
              "             'convincing': 825,\n",
              "             'hardly': 826,\n",
              "             'lame': 827,\n",
              "             'act': 828,\n",
              "             'former': 829,\n",
              "             'material': 830,\n",
              "             'appears': 831,\n",
              "             'understand': 832,\n",
              "             'twist': 833,\n",
              "             'episodes': 834,\n",
              "             'buy': 835,\n",
              "             'secret': 836,\n",
              "             'richard': 837,\n",
              "             'south': 838,\n",
              "             'bourne': 839,\n",
              "             'deal': 840,\n",
              "             'musical': 841,\n",
              "             'words': 842,\n",
              "             'unique': 843,\n",
              "             'mess': 844,\n",
              "             'opening': 845,\n",
              "             'society': 846,\n",
              "             'avoid': 847,\n",
              "             'footage': 848,\n",
              "             'joe': 849,\n",
              "             'free': 850,\n",
              "             'forget': 851,\n",
              "             'herself': 852,\n",
              "             'appear': 853,\n",
              "             'obvious': 854,\n",
              "             'box': 855,\n",
              "             'single': 856,\n",
              "             'average': 857,\n",
              "             'indian': 858,\n",
              "             'rent': 859,\n",
              "             'okay': 860,\n",
              "             'scary': 861,\n",
              "             'within': 862,\n",
              "             'office': 863,\n",
              "             'crime': 864,\n",
              "             'science': 865,\n",
              "             '80': 866,\n",
              "             'believable': 867,\n",
              "             'period': 868,\n",
              "             'showing': 869,\n",
              "             'call': 870,\n",
              "             'return': 871,\n",
              "             'keeps': 872,\n",
              "             'lee': 873,\n",
              "             'expected': 874,\n",
              "             'stay': 875,\n",
              "             'middle': 876,\n",
              "             'jack': 877,\n",
              "             'hands': 878,\n",
              "             'david': 879,\n",
              "             'attempts': 880,\n",
              "             'strong': 881,\n",
              "             'tension': 882,\n",
              "             'crew': 883,\n",
              "             'hilarious': 884,\n",
              "             'grade': 885,\n",
              "             'outside': 886,\n",
              "             'means': 887,\n",
              "             'viewing': 888,\n",
              "             'sadly': 889,\n",
              "             'hell': 890,\n",
              "             'whatever': 891,\n",
              "             'sorry': 892,\n",
              "             'recently': 893,\n",
              "             'stage': 894,\n",
              "             'decides': 895,\n",
              "             'hear': 896,\n",
              "             'team': 897,\n",
              "             'learn': 898,\n",
              "             'nor': 899,\n",
              "             'open': 900,\n",
              "             'break': 901,\n",
              "             'question': 902,\n",
              "             'remake': 903,\n",
              "             'porn': 904,\n",
              "             'pain': 905,\n",
              "             'imagine': 906,\n",
              "             'deep': 907,\n",
              "             'zombie': 908,\n",
              "             'basically': 909,\n",
              "             'killing': 910,\n",
              "             'company': 911,\n",
              "             'poorly': 912,\n",
              "             'dr.': 913,\n",
              "             'predictable': 914,\n",
              "             'taking': 915,\n",
              "             'large': 916,\n",
              "             'language': 917,\n",
              "             'giving': 918,\n",
              "             'public': 919,\n",
              "             'audiences': 920,\n",
              "             'ask': 921,\n",
              "             'cool': 922,\n",
              "             'america': 923,\n",
              "             'slasher': 924,\n",
              "             'west': 925,\n",
              "             'mentioned': 926,\n",
              "             'die': 927,\n",
              "             'christmas': 928,\n",
              "             'complete': 929,\n",
              "             'needed': 930,\n",
              "             'martin': 931,\n",
              "             'cgi': 932,\n",
              "             'boys': 933,\n",
              "             'vargas': 934,\n",
              "             'usual': 935,\n",
              "             'begin': 936,\n",
              "             'dad': 937,\n",
              "             'total': 938,\n",
              "             'somehow': 939,\n",
              "             'stick': 940,\n",
              "             'shame': 941,\n",
              "             'successful': 942,\n",
              "             'sitting': 943,\n",
              "             'fred': 944,\n",
              "             'meets': 945,\n",
              "             'unless': 946,\n",
              "             'dancing': 947,\n",
              "             'sounds': 948,\n",
              "             'above': 949,\n",
              "             'elements': 950,\n",
              "             'whose': 951,\n",
              "             'german': 952,\n",
              "             'considering': 953,\n",
              "             'caught': 954,\n",
              "             'credit': 955,\n",
              "             'interested': 956,\n",
              "             'move': 957,\n",
              "             'filming': 958,\n",
              "             'truth': 959,\n",
              "             'eventually': 960,\n",
              "             'share': 961,\n",
              "             'ability': 962,\n",
              "             'meaning': 963,\n",
              "             'agent': 964,\n",
              "             'fast': 965,\n",
              "             'stand': 966,\n",
              "             'onto': 967,\n",
              "             'plain': 968,\n",
              "             'comment': 969,\n",
              "             'kept': 970,\n",
              "             'situation': 971,\n",
              "             'setting': 972,\n",
              "             'value': 973,\n",
              "             'willing': 974,\n",
              "             'realize': 975,\n",
              "             'acted': 976,\n",
              "             'weird': 977,\n",
              "             'alive': 978,\n",
              "             'fairly': 979,\n",
              "             'dream': 980,\n",
              "             'building': 981,\n",
              "             'hair': 982,\n",
              "             'bored': 983,\n",
              "             'minute': 984,\n",
              "             'emotional': 985,\n",
              "             'directing': 986,\n",
              "             'theatrical': 987,\n",
              "             'famous': 988,\n",
              "             'begins': 989,\n",
              "             'front': 990,\n",
              "             'catch': 991,\n",
              "             'sequence': 992,\n",
              "             'runs': 993,\n",
              "             'follows': 994,\n",
              "             'song': 995,\n",
              "             'government': 996,\n",
              "             'miss': 997,\n",
              "             'actual': 998,\n",
              "             'makers': 999,\n",
              "             ...})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7AMykvoTKbZ",
        "colab_type": "text"
      },
      "source": [
        "Let's test that a non-word maps to xxunk:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYKd70BZTKba",
        "colab_type": "code",
        "outputId": "cc8fbf6c-2bed-475b-b1cc-d78ae9543e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "reviews_small.vocab.itos[reviews_small.vocab.stoi['rrachell']]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xxunk'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUeWmiJaTKbf",
        "colab_type": "code",
        "outputId": "189b4771-a12e-4667-e7d0-cedf182584e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "reviews_small.vocab.itos[reviews_small.vocab.stoi['language']]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'language'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfwSoSsATKbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = reviews_small.train[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eer_gzPMTKbk",
        "colab_type": "code",
        "outputId": "a006407c-7524-4a77-da82-6b411f6dda60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "t.data[:30]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,    5, 4619,   25,    0,   25,  867,   52,    5, 3776,    5, 1800,   95,   37,   85,  191,   64,  935,\n",
              "          0, 2738,  517,   18,   21,   11,   84, 2417,  192,   88, 3777,   64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFWl1JEFTKbn",
        "colab_type": "text"
      },
      "source": [
        "## Creating our term-document matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noK-HWQSTKbo",
        "colab_type": "text"
      },
      "source": [
        "As we covered in the last lesson, a term-document matrix represents a document as a \"bag of words\", that is, we don't keep track of the order the words are in, just which words occur (and how often)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWXdTj-NTKbo",
        "colab_type": "text"
      },
      "source": [
        "In the previous lesson, we used [sklearn's CountVectorizer](https://github.com/scikit-learn/scikit-learn/blob/55bf5d9/sklearn/feature_extraction/text.py#L940).  Today we will create our own (similar) version.  This is for two reasons:\n",
        "- to understand what sklearn is doing underneath the hood\n",
        "- to create something that will work with a fastai TextList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VATWsalTKbp",
        "colab_type": "text"
      },
      "source": [
        "To create our term-document matrix, we first need to learn about **counters** and **sparse matrices**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF_XokBRTKbq",
        "colab_type": "text"
      },
      "source": [
        "### Counters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOYD0Jr3TKbr",
        "colab_type": "text"
      },
      "source": [
        "Counters are a useful Python object.  If you aren't familar with them, here is how they work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Im2KEUSTKbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = Counter([4,2,8,8,4,8])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BBNf7aWTKbv",
        "colab_type": "code",
        "outputId": "f014798a-2fa6-4c4c-e55f-86c675adbed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "c"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({2: 1, 4: 2, 8: 3})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUOS1v0STKb1",
        "colab_type": "code",
        "outputId": "2bd97037-b3ef-4c6a-85c6-a3efff6b3039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "c"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({2: 1, 4: 2, 8: 3})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x_yTNs6TKb-",
        "colab_type": "text"
      },
      "source": [
        "Counters are from the collections module (along with OrderedDict, defaultdict, deque, and namedtuple)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-jztW_rTKcK",
        "colab_type": "text"
      },
      "source": [
        "### Sparse Matrices (in Scipy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86FkplrITKcM",
        "colab_type": "text"
      },
      "source": [
        "Even though we've reduced over 19,000 words down to 6,000, that is still a lot! Most tokens don't appear in most reviews.  We want to take advantage of this by storing our data as a **sparse matrix**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRfYdJruTKcN",
        "colab_type": "text"
      },
      "source": [
        "A matrix with lots of zeros is called **sparse** (the opposite of sparse is **dense**).  For sparse matrices, you can save a lot of memory by only storing the non-zero values.\n",
        "\n",
        "<img src=\"https://github.com/javiber/course-nlp/blob/master/images/sparse.png?raw=1\" alt=\"floating point\" style=\"width: 30%\"/>\n",
        "\n",
        "Another example of a large, sparse matrix:\n",
        "\n",
        "<img src=\"https://github.com/javiber/course-nlp/blob/master/images/Finite_element_sparse_matrix.png?raw=1\" alt=\"floating point\" style=\"width: 30%\"/>\n",
        "\n",
        "[Source](https://commons.wikimedia.org/w/index.php?curid=2245335)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nizXol2fTKcO",
        "colab_type": "text"
      },
      "source": [
        "There are the most common sparse storage formats:\n",
        "- coordinate-wise (scipy calls COO)\n",
        "- compressed sparse row (CSR)\n",
        "- compressed sparse column (CSC)\n",
        "\n",
        "Let's walk through [these examples](http://www.mathcs.emory.edu/~cheung/Courses/561/Syllabus/3-C/sparse.html)\n",
        "\n",
        "There are actually [many more formats](http://www.cs.colostate.edu/~mcrob/toolbox/c++/sparseMatrix/sparse_matrix_compression.html) as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSoXQoZyTKcP",
        "colab_type": "text"
      },
      "source": [
        "A class of matrices (e.g, diagonal) is generally called sparse if the number of non-zero elements is proportional to the number of rows (or columns) instead of being proportional to the product rows x columns.\n",
        "\n",
        "**Scipy Implementation**\n",
        "\n",
        "From the [Scipy Sparse Matrix Documentation](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html)\n",
        "\n",
        "- To construct a matrix efficiently, use either dok_matrix or lil_matrix. The lil_matrix class supports basic slicing and fancy indexing with a similar syntax to NumPy arrays. As illustrated below, the COO format may also be used to efficiently construct matrices\n",
        "- To perform manipulations such as multiplication or inversion, first convert the matrix to either CSC or CSR format.\n",
        "- All conversions among the CSR, CSC, and COO formats are efficient, linear-time operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg7aTPcJTKcQ",
        "colab_type": "text"
      },
      "source": [
        "### Our version of CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI18-yKDTKcR",
        "colab_type": "code",
        "outputId": "b9032e69-2670-4b54-e356-f7e38dedd432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Counter((reviews_small.valid.x)[0].data)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 32,\n",
              "         2: 1,\n",
              "         5: 32,\n",
              "         6: 1,\n",
              "         9: 10,\n",
              "         10: 7,\n",
              "         11: 10,\n",
              "         12: 1,\n",
              "         13: 4,\n",
              "         14: 6,\n",
              "         15: 6,\n",
              "         16: 4,\n",
              "         18: 2,\n",
              "         20: 1,\n",
              "         21: 3,\n",
              "         23: 1,\n",
              "         24: 3,\n",
              "         25: 2,\n",
              "         26: 1,\n",
              "         27: 3,\n",
              "         30: 1,\n",
              "         44: 1,\n",
              "         45: 1,\n",
              "         49: 1,\n",
              "         50: 3,\n",
              "         52: 1,\n",
              "         54: 2,\n",
              "         58: 1,\n",
              "         59: 1,\n",
              "         63: 2,\n",
              "         71: 1,\n",
              "         74: 1,\n",
              "         77: 1,\n",
              "         84: 1,\n",
              "         109: 1,\n",
              "         115: 1,\n",
              "         149: 1,\n",
              "         189: 1,\n",
              "         194: 1,\n",
              "         197: 2,\n",
              "         204: 1,\n",
              "         207: 1,\n",
              "         221: 1,\n",
              "         239: 1,\n",
              "         251: 1,\n",
              "         258: 1,\n",
              "         285: 1,\n",
              "         288: 1,\n",
              "         319: 1,\n",
              "         324: 1,\n",
              "         337: 1,\n",
              "         358: 1,\n",
              "         378: 1,\n",
              "         404: 1,\n",
              "         409: 1,\n",
              "         430: 1,\n",
              "         456: 1,\n",
              "         478: 1,\n",
              "         541: 1,\n",
              "         571: 1,\n",
              "         579: 1,\n",
              "         594: 1,\n",
              "         605: 1,\n",
              "         608: 1,\n",
              "         666: 2,\n",
              "         710: 1,\n",
              "         736: 1,\n",
              "         748: 1,\n",
              "         850: 1,\n",
              "         966: 1,\n",
              "         1057: 1,\n",
              "         1107: 1,\n",
              "         1331: 1,\n",
              "         1335: 1,\n",
              "         1346: 1,\n",
              "         1833: 1,\n",
              "         1843: 1,\n",
              "         1902: 1,\n",
              "         2110: 1,\n",
              "         2420: 1,\n",
              "         2595: 1,\n",
              "         2601: 1,\n",
              "         2705: 1,\n",
              "         2743: 1,\n",
              "         2764: 1,\n",
              "         2817: 1,\n",
              "         2875: 1,\n",
              "         3429: 1,\n",
              "         3480: 1,\n",
              "         3963: 1,\n",
              "         4051: 1,\n",
              "         4126: 1,\n",
              "         4736: 1,\n",
              "         5035: 1,\n",
              "         5821: 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ5Nfc4GTKcV",
        "colab_type": "code",
        "outputId": "c27b3d4e-d824-40eb-d9f1-dd4d0dd45f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "reviews_small.vocab.itos[6]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xxup'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDkQjE3ZTKcY",
        "colab_type": "code",
        "outputId": "76f625b1-e15f-452d-e1aa-b3ef70edf0be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "(reviews_small.valid.x)[1]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos i saw this movie once as a kid on the late - late show and fell in love with it . \n",
              " \n",
              "  xxmaj it took 30 + years , but i recently did find it on xxup dvd - it was n't cheap , either - in a xxunk that xxunk in war movies . xxmaj we watched it last night for the first time . xxmaj the audio was good , however it was grainy and had the trailers between xxunk . xxmaj even so , it was better than i remembered it . i was also impressed at how true it was to the play . \n",
              " \n",
              "  xxmaj the xxunk is around here xxunk . xxmaj if you 're xxunk in finding it , fire me a xxunk and i 'll see if i can get you the xxunk . xxunk"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU1SbHusTKcc",
        "colab_type": "code",
        "outputId": "955ca249-50bb-47d3-f077-cce8de65c74c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "(reviews_small.valid.x)[0]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos xxmaj this very funny xxmaj british comedy shows what might happen if a section of xxmaj london , in this case xxmaj xxunk , were to xxunk itself independent from the rest of the xxup uk and its laws , xxunk & post - war xxunk . xxmaj merry xxunk is what would happen . \n",
              " \n",
              "  xxmaj the explosion of a wartime bomb leads to the xxunk of ancient xxunk which show that xxmaj xxunk was xxunk to the xxmaj xxunk of xxmaj xxunk xxunk ago , a small historical xxunk long since forgotten . xxmaj to the new xxmaj xxunk , however , this is an unexpected opportunity to live as they please , free from any xxunk from xxmaj xxunk . \n",
              " \n",
              "  xxmaj stanley xxmaj xxunk is excellent as the minor city xxunk who suddenly finds himself leading one of the world 's xxunk xxunk . xxmaj xxunk xxmaj margaret xxmaj xxunk is a delight as the history professor who sides with xxmaj xxunk . xxmaj others in the stand - out cast include xxmaj xxunk xxmaj xxunk , xxmaj paul xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj xxunk & xxmaj sir xxmaj michael xxmaj xxunk . \n",
              " \n",
              "  xxmaj welcome to xxmaj xxunk !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvpCjfySTKcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_term_doc_matrix(label_list, vocab_len):\n",
        "    j_indices = []\n",
        "    indptr = []\n",
        "    values = []\n",
        "    indptr.append(0)\n",
        "\n",
        "    for i, doc in enumerate(label_list):\n",
        "        feature_counter = Counter(doc.data)\n",
        "        j_indices.extend(feature_counter.keys())\n",
        "        values.extend(feature_counter.values())\n",
        "        indptr.append(len(j_indices))\n",
        "        \n",
        "#     return (values, j_indices, indptr)\n",
        "\n",
        "    return scipy.sparse.csr_matrix((values, j_indices, indptr),\n",
        "                                   shape=(len(indptr) - 1, vocab_len),\n",
        "                                   dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puNgiIcsTKcj",
        "colab_type": "code",
        "outputId": "f94f4102-2d28-4426-cd0a-84ccd904dc5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%%time\n",
        "val_term_doc_small = get_term_doc_matrix(reviews_small.valid.x, len(reviews_small.vocab.itos))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 51.8 ms, sys: 8.1 ms, total: 59.9 ms\n",
            "Wall time: 56.4 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "XCi7fLHCTKcq",
        "colab_type": "code",
        "outputId": "1e32ab0a-6569-4c62-f8c9-69dedcdc5331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%%time\n",
        "trn_term_doc_small = get_term_doc_matrix(reviews_small.train.x, len(reviews_small.vocab.itos))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 203 ms, sys: 14.4 ms, total: 217 ms\n",
            "Wall time: 201 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQQL5tu1TKcu",
        "colab_type": "code",
        "outputId": "147e7c9e-2c22-4a47-d55c-8a9457ddf089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "trn_term_doc_small.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 6008)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr2B90AsTKcx",
        "colab_type": "code",
        "outputId": "8ed0ea9b-1974-46fe-c326-fbb3c1f50c33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "trn_term_doc_small[:,-10:]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<800x10 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 10 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW12PBKRTKc0",
        "colab_type": "code",
        "outputId": "43bac005-adfb-466f-ee33-9e2a871f8b7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "val_term_doc_small.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 6008)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cbVPYm0TKc3",
        "colab_type": "text"
      },
      "source": [
        "### More data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVV7oeldTKc4",
        "colab_type": "text"
      },
      "source": [
        "We could convert our sparse matrix to a dense matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYHXqozUTKc4",
        "colab_type": "code",
        "outputId": "991a2543-7b24-48b6-9d44-089a9da94244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "reviews_small.vocab.itos[-1:]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sollett']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uVc424BOTKc6",
        "colab_type": "code",
        "outputId": "c0fb77b4-6381-4155-f295-781443f58d83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "val_term_doc_small.todense()[:10,:10]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[32,  0,  1,  0, ...,  1,  0,  0, 10],\n",
              "        [ 9,  0,  1,  0, ...,  1,  0,  0,  7],\n",
              "        [ 6,  0,  1,  0, ...,  0,  0,  0, 12],\n",
              "        [78,  0,  1,  0, ...,  0,  0,  0, 44],\n",
              "        ...,\n",
              "        [ 8,  0,  1,  0, ...,  0,  0,  0,  8],\n",
              "        [43,  0,  1,  0, ...,  8,  1,  0, 25],\n",
              "        [ 7,  0,  1,  0, ...,  1,  0,  0,  9],\n",
              "        [19,  0,  1,  0, ...,  2,  0,  0,  5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo6ytI_5TKc8",
        "colab_type": "code",
        "outputId": "1e7862f1-68d1-49c9-c924-2bc46f937722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "reviews_small.vocab.itos[3]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xxeos'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPMkWpZDTKdB",
        "colab_type": "code",
        "outputId": "412f1481-9781-47f1-f86b-13d72523d470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "review = reviews_small.valid.x[1]; review"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos i saw this movie once as a kid on the late - late show and fell in love with it . \n",
              " \n",
              "  xxmaj it took 30 + years , but i recently did find it on xxup dvd - it was n't cheap , either - in a xxunk that xxunk in war movies . xxmaj we watched it last night for the first time . xxmaj the audio was good , however it was grainy and had the trailers between xxunk . xxmaj even so , it was better than i remembered it . i was also impressed at how true it was to the play . \n",
              " \n",
              "  xxmaj the xxunk is around here xxunk . xxmaj if you 're xxunk in finding it , fire me a xxunk and i 'll see if i can get you the xxunk . xxunk"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbg8PY3DTKdE",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** Since the word \"late\" shows up twice in this review (\"...as a kid on the late - late show...\"), confirm that a value of 2 is stored in the term-document matrix, for the row corresponding to this review and the column corresponding to the word \"late\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BINhcoroTKdF",
        "colab_type": "text"
      },
      "source": [
        "#### Answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLoHV0qOTKdF",
        "colab_type": "code",
        "outputId": "06e19447-26e3-4d05-8cf5-9e980d9e0bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Exercise: Confirm this\n",
        "val_term_doc_small[1, reviews_small.vocab.stoi['late']] == 2"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKlQT-j8TKdI",
        "colab_type": "code",
        "outputId": "4308311f-ba07-4aa0-93e5-3a94841322f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "val_term_doc_small"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<200x6008 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 27848 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2NlcWaWTKdK",
        "colab_type": "code",
        "outputId": "ad0fa9a5-7146-45dd-9fe6-c6093200afb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "val_term_doc_small[1]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x6008 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 81 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKVHRAO6TKdM",
        "colab_type": "code",
        "outputId": "f18d53ac-cbc4-4c91-e4dd-126889b5ba3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "val_term_doc_small[1].sum()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkPIkeGTTKdQ",
        "colab_type": "text"
      },
      "source": [
        "The review has 81 distinct tokens in it, and 144 tokens total."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-NmWjpxTKdQ",
        "colab_type": "code",
        "outputId": "594acfe2-d3a9-4f72-f7be-a9685fa1ad7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "review.data"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2,  19, 248,  21, ...,   9,   0,  10,   0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQqKcQiCTKdU",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** How could you convert review.data back to text (without just using review.text)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix4_N-VsTKdV",
        "colab_type": "text"
      },
      "source": [
        "#### Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRqIQNFRTKdW",
        "colab_type": "code",
        "outputId": "5b9b103e-2070-415e-f23d-77ed8f51cfd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Exercise\n",
        "\n",
        "' '.join([reviews_small.vocab.itos[a] for a in review.data])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"xxbos i saw this movie once as a kid on the late - late show and fell in love with it . \\n \\n  xxmaj it took 30 + years , but i recently did find it on xxup dvd - it was n't cheap , either - in a xxunk that xxunk in war movies . xxmaj we watched it last night for the first time . xxmaj the audio was good , however it was grainy and had the trailers between xxunk . xxmaj even so , it was better than i remembered it . i was also impressed at how true it was to the play . \\n \\n  xxmaj the xxunk is around here xxunk . xxmaj if you 're xxunk in finding it , fire me a xxunk and i 'll see if i can get you the xxunk . xxunk\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es0OfkFaTKdY",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: Confirm that review has 81 distinct tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmPLcI1qTKdY",
        "colab_type": "text"
      },
      "source": [
        "#### Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSxq8896TKdZ",
        "colab_type": "code",
        "outputId": "81362a7e-8974-4ec2-cdd2-c7da21459821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Exercise\n",
        "\n",
        "len(set(review.data)) == 81"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Aopi-ATKdc",
        "colab_type": "code",
        "outputId": "61949b4f-8cc9-496a-94d9-61a6f9b55769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "reviews_small.vocab.itos[1000:1020]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['state',\n",
              " 'street',\n",
              " 'impossible',\n",
              " 'clever',\n",
              " 'development',\n",
              " 'concept',\n",
              " 'william',\n",
              " 'worked',\n",
              " 'adventure',\n",
              " 'church',\n",
              " 'unlike',\n",
              " 'hold',\n",
              " 'lots',\n",
              " 'premise',\n",
              " 'shooting',\n",
              " 'washington',\n",
              " 'sick',\n",
              " 'effect',\n",
              " 'waiting',\n",
              " 'singing']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoE9mHejTKde",
        "colab_type": "text"
      },
      "source": [
        "`stoi` (string-to-int) is larger than `itos` (int-to-string)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcpUS7guTKde",
        "colab_type": "code",
        "outputId": "16599500-5fd1-4e27-c146-16879ded7aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(reviews_small.vocab.stoi) - len(reviews_small.vocab.itos)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVBrlW9LTKdh",
        "colab_type": "text"
      },
      "source": [
        "This is because many words are mapping to unknown.  We can confirm here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YytfsBBoTKdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unk = []\n",
        "for word, num in reviews_small.vocab.stoi.items():\n",
        "    if num==0:\n",
        "        unk.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmTh9aQ6TKdj",
        "colab_type": "code",
        "outputId": "256fe7e9-edca-44fa-ad80-4a765ee75f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(unk)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sEoQINEJTKdl",
        "colab_type": "code",
        "outputId": "f9b74ada-a487-41b0-f1c0-49fa7123a031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "unk[:100]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xxunk',\n",
              " 'bleeping',\n",
              " 'pert',\n",
              " 'ticky',\n",
              " 'schtick',\n",
              " 'whoosh',\n",
              " 'banzai',\n",
              " 'chill',\n",
              " 'wooofff',\n",
              " 'cheery',\n",
              " 'superstars',\n",
              " 'fashionable',\n",
              " 'cruelly',\n",
              " 'separating',\n",
              " 'mistreat',\n",
              " 'tensions',\n",
              " 'religions',\n",
              " 'baseness',\n",
              " 'nobility',\n",
              " 'puro',\n",
              " 'disowned',\n",
              " 'option',\n",
              " 'faults',\n",
              " 'dignified',\n",
              " 'realisation',\n",
              " 'reconciliation',\n",
              " 'mrs',\n",
              " 'iyer',\n",
              " 'heartbreaking',\n",
              " 'histories',\n",
              " 'frankness',\n",
              " 'starters',\n",
              " 'montage',\n",
              " 'swearing',\n",
              " 'halestorm',\n",
              " 'korea',\n",
              " 'concentrate',\n",
              " 'pic',\n",
              " 'elude',\n",
              " 'characteristics',\n",
              " 'blathered',\n",
              " 'brassed',\n",
              " 'declaration',\n",
              " 'peck',\n",
              " 'garnered',\n",
              " 'fearless',\n",
              " 'tempered',\n",
              " 'humane',\n",
              " 'tails',\n",
              " 'slighted',\n",
              " 'slater',\n",
              " 'barrage',\n",
              " 'underway',\n",
              " 'operating',\n",
              " 'tag',\n",
              " 'dorff',\n",
              " 'reid',\n",
              " 'continually',\n",
              " 'revel',\n",
              " 'nra',\n",
              " 'benton',\n",
              " 'slate',\n",
              " 'penal',\n",
              " 'vengeful',\n",
              " 'seed',\n",
              " 'backbone',\n",
              " 'dismal',\n",
              " 'fortunate',\n",
              " 'ds',\n",
              " 'tmob',\n",
              " 'autographed',\n",
              " 'intercepted',\n",
              " 'lectured',\n",
              " 'reprints',\n",
              " 'comicon',\n",
              " 'attendees',\n",
              " 'blackhawk',\n",
              " 'insisted',\n",
              " 'jumped',\n",
              " 'apologized',\n",
              " 'wishing',\n",
              " 'seller',\n",
              " 'abomination',\n",
              " 'crib',\n",
              " 'seriousness',\n",
              " 'reclaim',\n",
              " 'sidenotes',\n",
              " 'archenemy',\n",
              " 'simultaneous',\n",
              " 'sheet',\n",
              " 'ely',\n",
              " 'leaping',\n",
              " 'brick',\n",
              " 'blasting',\n",
              " 'pistol',\n",
              " 'duster',\n",
              " 'postscript',\n",
              " 'accompanied',\n",
              " '1975',\n",
              " 'smack']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9hHWmT3TKdp",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3-23E4sTKdp",
        "colab_type": "text"
      },
      "source": [
        "We define the **log-count ratio** $r$ for each word $f$:\n",
        "\n",
        "$r = \\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text{ratio of feature $f$ in negative documents}}$\n",
        "\n",
        "where ratio of feature $f$ in positive documents is the number of times a positive document has a feature divided by the number of positive documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0cy85CkTKdq",
        "colab_type": "code",
        "outputId": "4bbd2664-3d90-40ec-a5de-3f54f93b9e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "reviews_small.y.classes"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['negative', 'positive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZAWrT0oTKds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = trn_term_doc_small\n",
        "y = reviews_small.train.y\n",
        "val_y = reviews_small.valid.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbgVIsjGTKdu",
        "colab_type": "code",
        "outputId": "a582da7a-219e-4b7e-f273-f885249b2a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "positive = y.c2i['positive']\n",
        "negative = y.c2i['negative']\n",
        "positive, negative"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzYI45SpfNVb",
        "colab_type": "code",
        "outputId": "d8b7a56b-1306-4a41-ec80-bdb97adf3c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x.shape, y.items.shape"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((800, 6008), (800,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMTzIX6jTKd5",
        "colab_type": "code",
        "outputId": "fe25564e-f746-490b-c276-71803e04a88f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7154,    0,  417,    0, ...,    0,    3,    3,    3], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIBcEDLATKd7",
        "colab_type": "code",
        "outputId": "77fd7fe3-b3f2-4f5c-8583-a95b2fdee15a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.asarray(x[y.items==positive].sum(0))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6471,    0,  383,    0, ...,    3,    0,    0,    0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaJY0HYnTKd-",
        "colab_type": "code",
        "outputId": "6c98d40a-2342-4d17-c6c1-0a2fdd817548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.squeeze(np.asarray(x[y.items==positive].sum(0)))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6471,    0,  383,    0, ...,    3,    0,    0,    0], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZgyXSbbTKeB",
        "colab_type": "text"
      },
      "source": [
        "For each word in our vocabulary, we are summing up how many positive reviews it is in, and how many negative reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Czznp7QTKeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))\n",
        "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKxXDTXjTKeG",
        "colab_type": "code",
        "outputId": "87db56ba-75c8-443c-c1f9-79383c1a574c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "p1[:10]"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6471,     0,   383,     0,     0, 10267,   674,    57,     0,  5260], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--5C_F3GTKeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v = reviews_small.vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vObuqkdxTKeM",
        "colab_type": "text"
      },
      "source": [
        "### Using our ratios for even more data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs_cfuhfTKeM",
        "colab_type": "text"
      },
      "source": [
        "We can use p0 and p1 to do some more data exploration!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxCwArQ6TKeN",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: compare how often \"loved\" appears in positive reviews vs. negative reviews.  How about \"hate\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f83JUZJTKeN",
        "colab_type": "text"
      },
      "source": [
        "#### Answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVULKTNnTKeO",
        "colab_type": "code",
        "outputId": "4f01c1fc-88ba-4821-a682-43f57e667326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Exercise: How often does the word \"loved\" appear in neg vs. pos reviews?\n",
        "p0[v.stoi['loved']], p1[v.stoi['loved']] "
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRiQMv67TKeP",
        "colab_type": "code",
        "outputId": "c9d99a9a-5215-43a5-d579-032be8918a85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Exercise: How often does the word \"hated\" appear in neg vs. pos reviews?\n",
        "p0[v.stoi['hated']], p1[v.stoi['hated']] "
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUTqAONkTKeR",
        "colab_type": "text"
      },
      "source": [
        "#### positive reviews with the word \"hated\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnbuIDciTKeS",
        "colab_type": "text"
      },
      "source": [
        "I was curious to look at an example of a postive review with the word \"hated\" in it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgRLh6Y7TKeS",
        "colab_type": "code",
        "outputId": "735dc199-19c2-4a7a-9bd5-32f0fc3ce9e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "v.stoi['hated']"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1977"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0u_j7kCTKeW",
        "colab_type": "code",
        "outputId": "fdd3343d-097b-478f-908a-ee0d3cb4ae90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "a = np.argwhere((x[:,1977] > 0))[:,0]; a"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 15,  49, 304, 351, 393, 612, 695, 773], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHJyAXhOTKeX",
        "colab_type": "code",
        "outputId": "9d62342d-9f57-49b8-9aa8-3aa48a0ed707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "b = np.argwhere(y.items==positive)[:,0]; b"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,   3,  10,  11, ..., 787, 789, 790, 797])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqHNF0KLTKeZ",
        "colab_type": "code",
        "outputId": "a5ffca03-03ea-43fe-fb48-7901469e6c0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "set(a).intersection(set(b))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{393, 612, 695}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq4oBRvITKeb",
        "colab_type": "code",
        "outputId": "0128c86b-3571-4d3e-9449-2699178c5561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "review = reviews_small.train.x[695]\n",
        "review.text"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"xxbos xxmaj xxunk , yeah this episode is extremely underrated . \\n \\n  xxmaj even though there is a xxup lot of bad writing and acting at parts . i think the good over wins the bad . \\n \\n  i love the xxunk parts and the big ' twist ' at the end . i absolutely love that scene when xxmaj michelle xxunk xxmaj tony . xxmaj it 's actually one of my favorite scenes of xxmaj season 1 . \\n \\n  xxmaj for some reason , people have always hated the xxmaj xxunk episodes , yet i have always liked them . xxmaj they 're not the best , in terms of writing . but the theme really does interest me , \\n \\n  i 'm gon na give it a xxup three star , but if the writing were a little more consistent i 'd give it xxup four .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng_FADONTKed",
        "colab_type": "text"
      },
      "source": [
        "#### negative reviews with the word \"loved\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DllPvxcDTKed",
        "colab_type": "text"
      },
      "source": [
        "Now, let's look at an example of a negative review that contains the word \"loved\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJvHlRsaTKee",
        "colab_type": "code",
        "outputId": "8c092539-4cf0-4c02-dcbe-4357623cc26b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "v.stoi['loved']"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "535"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uU8KQWhTKef",
        "colab_type": "code",
        "outputId": "b071ff3d-277a-4ee8-c8bb-cd685862b28b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "a = np.argwhere((x[:,534] > 0))[:,0]; a"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,  19,  24,  51,  61,  70,  81, 110, 123, 155, 175, 193, 221, 265, 274, 279, 284, 290, 295, 304, 360, 384,\n",
              "       421, 465, 516, 520, 548, 569, 588, 604, 620, 631, 661, 672, 679, 702, 709, 759, 764, 792], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG8tboH6TKeg",
        "colab_type": "code",
        "outputId": "c865a287-e5be-4b64-8262-8c310d1d00da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "b = np.argwhere(y.items==negative)[:,0]; b"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   2,   4,   5, ..., 795, 796, 798, 799])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Q6NYO1XOTKei",
        "colab_type": "code",
        "outputId": "ee05bb5e-d717-4f84-da17-929f409624e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "set(a).intersection(set(b))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0,\n",
              " 24,\n",
              " 51,\n",
              " 70,\n",
              " 81,\n",
              " 123,\n",
              " 155,\n",
              " 193,\n",
              " 221,\n",
              " 274,\n",
              " 279,\n",
              " 284,\n",
              " 290,\n",
              " 295,\n",
              " 304,\n",
              " 421,\n",
              " 516,\n",
              " 548,\n",
              " 604,\n",
              " 620,\n",
              " 631,\n",
              " 672,\n",
              " 679,\n",
              " 709,\n",
              " 759,\n",
              " 764,\n",
              " 792}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3QB9LTlTKek",
        "colab_type": "code",
        "outputId": "1fb51c55-4244-4ba6-846c-3bef2e737341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "review = reviews_small.train.x[792]\n",
        "review.text"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xxbos xxmaj this is not really a zombie film , if we \\'re xxunk zombies as the dead walking around . xxmaj here the protagonist , xxmaj xxunk xxmaj louque ( played by an xxunk young xxmaj dean xxmaj xxunk ) , xxunk control of a method to create zombies , though in fact , his \\' method \\' is to mentally project his thoughts and control other living people \\'s minds turning them into xxunk slaves . xxmaj this is an interesting concept for a movie , and was done much more effectively by xxmaj xxunk xxmaj lang in his series of \\' xxmaj dr. xxmaj mabuse \\' films , including \\' xxmaj dr. xxmaj mabuse the xxmaj xxunk \\' ( xxunk ) and \\' xxmaj the xxmaj testament of xxmaj dr. xxmaj mabuse \\' ( 1933 ) . xxmaj here it is unfortunately xxunk to his quest to regain the love of his former fiancée , xxmaj claire xxmaj duvall ( played by the xxmaj anne xxmaj xxunk look alike with a bad xxunk , xxmaj dorothy xxmaj stone ) which is really the major theme . \\n \\n  xxmaj the movie has an intriguing beginning , as xxmaj louque is sent on a military xxunk expedition to xxmaj xxunk to end the cult of zombies that came from there . xxmaj at some type of compound ( where we get great 30s sets and clothes ) he xxunk his xxunk to xxmaj claire , and then barely five minutes later , she gives him back his ring xxunk her love for his pal , xxmaj xxunk xxmaj greyson ( xxmaj robert xxmaj xxunk ) . xxmaj it \\'s unintentionally funny the way they talk to each other without making eye contact . xxmaj this would have been a great movie for \\' xxmaj mystery xxmaj science xxmaj theater xxunk \\' , if they had n\\'t already xxunk it . \\n \\n  xxmaj it \\'s never shown how xxmaj louque actually learns the \\' xxunk \\' secret , but he then uses it to kill his enemies , create a giant army of xxunk carrying soldiers and body guards . xxmaj we wo n\\'t see such sheer force of will until xxmaj john xxmaj xxunk in \\' xxmaj the xxmaj brain xxmaj from xxmaj planet xxmaj xxunk \\' ( xxunk ) . \\n \\n  xxmaj finally xxmaj claire xxunk to marry him if he will let xxmaj greyson live and return to xxmaj america . xxmaj louque agrees , but actually turns him into one of his xxunk slaves . xxmaj on their wedding night he realizes that xxmaj claire will only begin to love him if he gives up his \\' powers . \\' xxmaj to gain her love , he does so , causing the \\' revolt \\' of the title , in which all his slaves xxunk and attack his compound and kill him . xxmaj greyson xxunk xxmaj claire , and we seem to be at the end of a parable : \" xxmaj whom the xxunk would destroy , they first make mad . \" \\n \\n  xxmaj so really then , it \\'s not that bad of a film , despite the low imdb rating it currently has . xxmaj on repeated viewings ( ? ) one can see the xxunk in the well formed script ! xxmaj dean xxmaj xxunk had yet to develop into a good actor , and is almost unrecognizable in his xxunk -- is that really his own hair ? xxmaj we remember him more for his xxunk , old man roles in \\' xxmaj white xxmaj christmas \\' ( xxunk ) , \\' x xxmaj the xxmaj unknown \\' ( 1956 ) and \\' xxmaj king xxmaj xxunk \\' ( 1958 ) . xxmaj the story xxunk a lot of its basic themes from the xxmaj xxunk brothers better , earlier film \\' xxmaj white xxmaj zombie \\' ( xxunk ) in which xxunk xxmaj robert xxmaj xxunk ( as xxmaj charles xxmaj xxunk ) uses \\' xxunk \\' to win the love of xxmaj xxunk xxmaj xxunk ( as xxmaj xxunk xxmaj parker ) . \\n \\n  xxmaj if you want real zombie movies ( of which there are hundreds ! ) i \\'d start with \\' xxmaj white xxmaj zombie \\' ( xxunk ) , \\' xxmaj king of the xxmaj zombies \\' ( xxunk ) , \\' i xxmaj walked with a xxmaj zombie \\' ( xxunk ) , \\' xxmaj night of the xxmaj living xxmaj dead \\' ( xxunk ) , \\' xxmaj the xxmaj last xxmaj man on xxmaj earth \\' ( 1964 ) and its two xxunk . xxmaj in the modern era of classy films , there are \\' xxmaj horror xxmaj express \\' ( 1972 ) , \\' xxmaj the xxmaj xxunk and the xxmaj xxunk \\' ( xxunk ) , \\' 28 xxmaj days xxmaj later \\' ( 2002 ) and its sequel , as well as many , many , others too numerous to mention . \\n \\n  xxmaj this one is not really a zombie film . xxmaj judging this movie on its own terms , it \\'s more of a semi - xxmaj gothic romance . xxmaj as such it ranks a little below some of xxmaj universal \\'s bottom billed b horror movies of the late 30s and early xxunk . xxmaj so i \\'ll give it a 5 .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJrp7hoBTKel",
        "colab_type": "text"
      },
      "source": [
        "## Applying Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAjsFlifTKem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))\n",
        "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRegF0iJTKen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
        "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCqp7ZDWTKeo",
        "colab_type": "code",
        "outputId": "924fe882-9cb1-4289-b1f9-5b0a252b2b99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "r = np.log(pr1/pr0); r"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.015487,  0.084839,  0.      ,  0.084839, ...,  1.471133, -1.301455, -1.301455, -1.301455])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfSLuqGeTKeq",
        "colab_type": "text"
      },
      "source": [
        "### Vocab most likely associated with positive/negative reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px0c6ZMPTKer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "biggest = np.argpartition(r, -10)[-10:]\n",
        "smallest = np.argpartition(r, 10)[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I_xWws5TKes",
        "colab_type": "text"
      },
      "source": [
        "Most positive words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBmh6uwxTKes",
        "colab_type": "code",
        "outputId": "fb62b8ca-c77b-4dcb-8a76-fa865717ab50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "[v.itos[k] for k in biggest]"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sport',\n",
              " 'davies',\n",
              " 'gilliam',\n",
              " 'fanfan',\n",
              " 'biko',\n",
              " 'felix',\n",
              " 'noir',\n",
              " 'jabba',\n",
              " 'astaire',\n",
              " 'jimmy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtbo-OcOTKet",
        "colab_type": "code",
        "outputId": "ca7150de-31a2-467a-d428-7171bc4a23e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.argmax(trn_term_doc_small[:,v.stoi['biko']])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "515"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVmrKATdTKex",
        "colab_type": "code",
        "outputId": "4ae35837-004b-4342-82a8-0005957fd964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "reviews_small.train.x[515]"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos \" xxmaj the xxmaj true xxmaj story xxmaj of xxmaj the xxmaj friendship xxmaj that xxmaj shook xxmaj south xxmaj africa xxmaj and xxmaj xxunk xxmaj the xxmaj world . \" \n",
              " \n",
              "  xxmaj richard xxmaj attenborough , who directed \" a xxmaj bridge xxmaj too xxmaj far \" and \" xxmaj gandhi \" , wanted to bring the story of xxmaj steve xxmaj biko to life , and the journey and trouble that xxunk xxmaj donald xxmaj woods went through in order to get his story told . xxmaj the films uses xxmaj wood 's two books for it 's information and basis - \" xxmaj biko \" and \" xxmaj asking for xxmaj trouble \" . \n",
              " \n",
              "  xxmaj the film takes place in the late 1970 's , in xxmaj south xxmaj africa . xxmaj south xxmaj africa is in the grip of the terrible apartheid , which keeps the blacks separated from the whites and xxunk the whites as the superior race . xxmaj the blacks are forced to live in xxunk on the xxunk of the cities and xxunk , and they come under frequent xxunk by the police and the army . xxmaj we are shown a dawn xxunk on a xxunk , as xxunk and armed police force their way through the camp beating and even killing the inhabitants . xxmaj then we are introduced to xxmaj donald xxmaj woods ( xxmaj kevin xxmaj kline ) , who is the editor of a popular newspaper . xxmaj after xxunk a negative story about black xxunk xxmaj steve xxmaj biko ( xxmaj denzel xxmaj washington ) , xxmaj woods goes to meet with him . xxmaj the two are xxunk of each other at first , but they soon become good friends and xxmaj biko shows the horrors of the apartheid system from a black persons point of view to xxmaj woods . xxmaj this xxunk xxmaj woods to speak out against what 's happening around him , and makes him desperate to bring xxmaj steve xxmaj biko 's story out of the xxunk of the white man 's xxmaj south xxmaj africa and to the world . xxmaj soon , xxmaj steve xxmaj biko is arrested and is killed in prison . xxmaj now xxmaj woods and his family are daring to escape from xxmaj south xxmaj africa to xxmaj england , where xxmaj woods can xxunk his book about xxmaj steve xxmaj biko and the apartheid . \n",
              " \n",
              "  xxmaj when i first heard of \" xxmaj cry xxmaj freedom \" , i was under the impression that it was a movie completely dedicated to the life of xxmaj steve xxmaj biko . i had never actually heard of xxmaj steve xxmaj biko before i seen this film , as the events in this film were really before my time . xxmaj but it 's more about the story of xxmaj donald xxmaj woods and his journey across the border into xxmaj xxunk as he tried to xxunk the xxmaj south xxmaj african xxunk . xxmaj woods was put on a five year type house xxunk after xxmaj steve xxmaj biko was killed . xxmaj so in order to xxunk his xxunk on xxmaj steve xxmaj biko , he had to escape . xxmaj because the xxunk would be considered xxunk in xxmaj south xxmaj africa and that could have resulted in xxmaj woods meeting a fate similar to that of xxmaj biko 's . xxmaj the real xxmaj donald xxmaj woods and his wife acted as xxunk to this film . \n",
              " \n",
              "  xxmaj denzel xxmaj washington is only in the film for the first hour , and i was disappointed with that as i was expecting to see him for the entire movie . xxmaj but he was amazing as xxmaj steve xxmaj biko , and captured his personality from what i 've read really well and his accent sounded perfect . xxmaj his performance earned him an xxmaj oscar nomination for xxmaj best xxmaj supporting xxmaj actor . xxmaj kevin xxmaj kline delivers a excellent and thought - xxunk performance as xxmaj donald xxmaj woods , and xxmaj penelope xxmaj xxunk is excellent as his wife xxmaj xxunk . \n",
              " \n",
              "  xxmaj filming took place in xxmaj xxunk , as needless to say problems xxunk when they tried to film it in xxmaj south xxmaj africa . xxmaj while in xxmaj south xxmaj africa , the xxmaj south xxmaj african xxunk followed the film crew everywhere , so they got the bad xxunk and they pulled out and went to xxunk xxmaj xxunk instead . xxmaj despite everything , and the fact that the apartheid did n't end ' xxunk seven years later , \" xxmaj cry xxmaj freedom \" was n't xxunk in xxmaj south xxmaj africa . xxmaj but xxunk showing the movie received bomb threats . \n",
              " \n",
              "  xxmaj richard xxmaj attenborough brings the horrors of the apartheid to the screen with extreme force and determination . xxmaj he does n't hold back at the end of the movie when showing what was supposed to be a xxunk xxunk by students in a xxunk , turns into a massacre when police open fire on them . xxmaj the film ends with the names of all the anti - apartheid xxunk who died in prison , and the explanations for their deaths . xxmaj many had \" xxmaj no xxmaj explanation \" . xxmaj quite a few were \" xxmaj xxunk \" , which is hard to believe , and many more either fell from the top of the xxunk or were \" xxmaj suicide from xxmaj hanging \" . xxmaj no one will ever know what really happened to them , but i think it 's fair to say that none of these men died at their own hands , but at the hands of others ; or to be more xxunk , at the hands of the police . \n",
              " \n",
              "  \" xxmaj cry xxmaj freedom \" is a must - see movie for it 's portrayal and story of xxmaj steve xxmaj biko . xxmaj it 's also a xxunk and xxunk portrayal of a beautiful land divided and in the xxunk grips of racial xxunk and violence ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "469fzau-TKey",
        "colab_type": "text"
      },
      "source": [
        "Most negative words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_kO4fzocTKez",
        "colab_type": "code",
        "outputId": "afe56e84-6612-4a24-a636-1225702339f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "[v.itos[k] for k in smallest]"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['worst',\n",
              " 'crap',\n",
              " 'crater',\n",
              " 'porn',\n",
              " 'disappointment',\n",
              " 'dog',\n",
              " 'vargas',\n",
              " 'naschy',\n",
              " 'fuqua',\n",
              " 'soderbergh']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkB2LLsDTKe0",
        "colab_type": "code",
        "outputId": "05b6daa8-2028-4b65-915f-e39439406b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.argmax(trn_term_doc_small[:,v.stoi['soderbergh']])"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "434"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndnqOD_XTKe2",
        "colab_type": "code",
        "outputId": "71c75944-d3ab-4e20-a625-ff2b7ceb9f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        }
      },
      "source": [
        "reviews_small.train.x[434]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj xxunk , after xxunk ) , i can xxunk join both xxunk of \" xxmaj at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \n",
              " \n",
              "  xxmaj it 's usually satisfying to watch a film director change his style / subject , but xxmaj soderbergh 's most recent stinker , xxmaj the xxmaj girlfriend xxmaj xxunk ) , was also missing a story , so narrative ( and editing ? ) seem to suddenly be xxmaj soderbergh 's main challenge . xxmaj strange , after xxunk years in the business . xxmaj he was probably never much good at narrative , just xxunk it well inside \" edgy \" projects . \n",
              " \n",
              "  xxmaj none of this excuses him this present , almost diabolical failure . xxmaj as xxmaj david xxmaj xxunk xxunk , \" two parts of xxmaj che do n't ( even ) make a whole \" . \n",
              " \n",
              "  xxmaj epic xxunk in name only , xxmaj che(2008 ) barely qualifies as a feature film ! xxmaj it certainly has no legs , xxunk as except for its xxunk ultimate resolution forced upon it by history , xxmaj soderbergh 's xxunk - long xxunk just goes nowhere . \n",
              " \n",
              "  xxmaj even xxmaj margaret xxmaj xxunk , the more xxunk of xxmaj australia 's xxmaj at xxmaj the xxmaj movies duo , noted about xxmaj soderbergh 's xxunk waste of ( xxup xxunk digital xxunk ) : \" you 're in the woods ... xxunk in the woods ... xxunk in the woods ... \" . i too am surprised xxmaj soderbergh did n't give us another xxunk of xxup that somewhere between his xxunk two xxmaj parts , because he still left out massive xxunk of xxmaj che 's \" xxunk \" life ! \n",
              " \n",
              "  xxmaj for a xxunk of an important but infamous historical figure , xxmaj soderbergh xxunk xxunk , if not deliberately insults , his audiences by \n",
              " \n",
              "  1 . never providing most of xxmaj che 's story ; \n",
              " \n",
              "  2 . xxunk xxunk film xxunk with mere xxunk xxunk ; \n",
              " \n",
              "  3 . xxunk both true xxunk and a narrative of events ; \n",
              " \n",
              "  4 . barely developing an idea , or a character ; \n",
              " \n",
              "  5 . remaining xxunk episodic ; \n",
              " \n",
              "  6 . xxunk proper context for scenes --- whatever we do get is xxunk in xxunk xxunk ; \n",
              " \n",
              "  7 . xxunk xxunk all audiences ( even xxmaj spanish - xxunk will be confused by the xxunk xxunk in xxmaj english ) ; and \n",
              " \n",
              "  8 . xxunk xxunk his main subject into one dimension . xxmaj why , at xxup this late stage ? xxmaj the t - shirt franchise has been a success ! \n",
              " \n",
              "  xxmaj our sense of xxunk is surely due to xxmaj peter xxmaj xxunk and xxmaj benjamin xxunk xxmaj xxunk xxunk their screenplay solely on xxmaj xxunk 's memoirs . xxmaj so , like a poor student who has read only xxup one of his xxunk xxunk for his xxunk , xxmaj soderbergh 's product is xxunk limited in perspective . \n",
              " \n",
              "  xxmaj the audience is held captive within the same xxunk knowledge , scenery and circumstances of the \" revolutionaries \" , but that does n't xxunk our sympathy . xxmaj instead , it xxunk on us that \" xxmaj ah , xxmaj soderbergh 's trying to xxunk his audiences the same as the xxmaj latino peasants were at the time \" . xxmaj but these are the xxup same illiterate xxmaj latino peasants who xxunk out the good doctor to his enemies . xxmaj why does xxmaj soderbergh feel the need to xxunk us with them , and keep us equally mentally captive ? xxmaj such audience xxunk must have a purpose . \n",
              " \n",
              "  xxmaj part2 is more xxunk than xxmaj part1 , but it 's literally mind - numbing with its repetitive bush - bashing , misery of xxunk , and lack of variety or character xxunk . deltoro 's xxmaj che has no opportunity to grow as a person while he struggles to xxunk his own ill - xxunk troops . xxmaj the only xxunk is the humour as xxmaj che deals with his sometimes deeply ignorant \" revolutionaries \" , some of whom xxunk lack self - control around local peasants or food . xxmaj we certainly get no insight into what caused the conditions , nor any xxunk xxunk of their xxunk xxunk , such as it was . \n",
              " \n",
              "  xxmaj part2 's xxunk xxunk remains xxunk episodic : again , nothing is telegraphed or xxunk . xxmaj thus even the scenes with xxmaj xxunk xxmaj xxunk ( xxmaj xxunk xxmaj xxunk ) are unexpected and disconcerting . xxmaj any xxunk events are portrayed xxunk and xxmaj latino - xxunk , with xxmaj part1 's interviews xxunk by time - xxunk xxunk between the corrupt xxmaj xxunk president ( xxmaj xxunk de xxmaj xxunk ) and xxup us xxmaj government xxunk promising xxup cia xxunk ( ! ) . \n",
              " \n",
              "  xxmaj the rest of xxmaj part2 's \" woods \" and day - for - night blue xxunk just xxunk the audience until they 're xxunk the xxunk . \n",
              " \n",
              "  xxmaj perhaps deltoro felt too xxunk the frustration of many non - xxmaj american xxmaj latinos about never getting a truthful , xxunk history of xxmaj che 's xxunk within their own countries . xxmaj when foreign xxunk still wo n't deliver a free press to their people -- for whatever reason -- then one can see how a popular xxmaj american indie producer might set out to xxunk the not - so - well - read ( \" i may not be able to read or write , but i 'm xxup not xxunk . xxmaj the xxmaj inspector xxmaj xxunk ) ) out to their own local xxunk . xxmaj the film 's obvious xxunk and gross over - xxunk hint very strongly that it 's aiming only at the xxunk of the less - informed xxup who xxup still xxup speak xxup little xxmaj english . xxmaj if they did , they 'd have read xxunk on the subject already , and xxunk the relevant social issues amongst themselves -- learning the lessons of history as they should . \n",
              " \n",
              "  xxmaj such insights are precisely what societies still need -- and not just the remaining illiterate xxmaj latinos of xxmaj central and xxmaj south xxmaj america -- yet it 's what xxmaj che(2008 ) xxunk fails to deliver . xxmaj soderbergh xxunk his lead because he 's weak on narrative . i am xxunk why xxmaj xxunk deltoro deliberately chose xxmaj soderbergh for this project if he knew this . xxmaj it 's been xxunk , xxunk about xxmaj xxunk was xxunk wanted : it 's what i went to see this film for , but the director xxunk robs us of that . \n",
              " \n",
              "  xxmaj david xxmaj xxunk , writing in xxmaj the xxmaj australian ( xxunk ) observed that while xxmaj part1 was \" uneven \" , xxmaj part2 actually \" goes rapidly downhill \" from there , \" xxunk xxmaj che 's final xxunk in xxmaj xxunk in xxunk detail \" , which \" ... feels almost unbearably slow and turgid \" . \n",
              " \n",
              "  xxmaj che : xxmaj the xxmaj xxunk aka xxmaj part2 is certainly no xxunk for xxmaj xxunk , painting it a picture of misery and xxunk . xxmaj the entire second half is only xxunk by the aforementioned humour , and the dramatic -- yet tragic -- capture and execution of the film 's subject . \n",
              " \n",
              "  xxmaj the rest of this xxunk cinema xxunk is just confusing , irritating misery -- xxunk , for a xxmaj soderbergh film , to be avoided at all costs . xxmaj it is bound to break the hearts of all who know even just a xxunk about the xxunk / 10 )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw-IkumBTKe3",
        "colab_type": "code",
        "outputId": "4bf6f926-bb59-4798-a0d8-f889b60b46d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "trn_term_doc_small[:,v.stoi['soderbergh']]"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<800x1 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 1 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vK_ArANgTKe5",
        "colab_type": "code",
        "outputId": "3ab6a748-e249-4d57-a5d1-6b678d70d4f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "[v.itos[k] for k in smallest]"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['worst',\n",
              " 'crap',\n",
              " 'crater',\n",
              " 'porn',\n",
              " 'disappointment',\n",
              " 'dog',\n",
              " 'vargas',\n",
              " 'naschy',\n",
              " 'fuqua',\n",
              " 'soderbergh']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBrzTUi_TKe8",
        "colab_type": "text"
      },
      "source": [
        "### Continuing with Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SNG4wZaTKe9",
        "colab_type": "code",
        "outputId": "d2bfa56f-e9cf-4797-8318-715c3b3fb515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(y.items==positive).mean(), (y.items==negative).mean()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.47875, 0.52125)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6H9XOSZTKe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = np.log((y.items==positive).mean() / (y.items==negative).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RkuyhjSTKe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = (val_term_doc_small @ r + b) > 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ8GjD89TKfA",
        "colab_type": "code",
        "outputId": "f27bce6d-5aac-481d-edbb-0ac69057e37e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(preds == val_y.items).mean()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.645"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYc0VzWvTKfB",
        "colab_type": "text"
      },
      "source": [
        "## Switching to full data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGfHT95JTKfC",
        "colab_type": "text"
      },
      "source": [
        "Now that we have our approach working on a smaller sample of the data, we can try using it on the full dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMblZY9XTKfC",
        "colab_type": "text"
      },
      "source": [
        "### Download data and process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWb2EvVkTKfD",
        "colab_type": "code",
        "outputId": "d1292f8f-c57d-4f52-db62-f1a5d44f93ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "path = untar_data(URLs.IMDB)\n",
        "path.ls()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/imdb/tmp_lm'),\n",
              " PosixPath('/root/.fastai/data/imdb/unsup'),\n",
              " PosixPath('/root/.fastai/data/imdb/README'),\n",
              " PosixPath('/root/.fastai/data/imdb/tmp_clas'),\n",
              " PosixPath('/root/.fastai/data/imdb/imdb.vocab'),\n",
              " PosixPath('/root/.fastai/data/imdb/test'),\n",
              " PosixPath('/root/.fastai/data/imdb/train')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9JuT4UrTKfF",
        "colab_type": "code",
        "outputId": "590697e9-ed1a-4e79-e561-feabdaac0e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "(path/'train').ls()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/imdb/train/unsupBow.feat'),\n",
              " PosixPath('/root/.fastai/data/imdb/train/neg'),\n",
              " PosixPath('/root/.fastai/data/imdb/train/pos'),\n",
              " PosixPath('/root/.fastai/data/imdb/train/labeledBow.feat')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIAlrioHTKfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_full = (TextList.from_folder(path)\n",
        "             #grab all the text files in path\n",
        "             .split_by_folder(valid='test')\n",
        "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
        "             .label_from_folder(classes=['neg', 'pos']))\n",
        "             #label them all with their folders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2QV749UTKfJ",
        "colab_type": "code",
        "outputId": "183c9ffc-ee26-4abb-be0b-55b6e17204dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(reviews_full.train), len(reviews_full.valid)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3pJ4XyPTKfK",
        "colab_type": "text"
      },
      "source": [
        "We will store the vocab in a variable `v` since we will be using it frequently:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3gpImsfTKfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v = reviews_full.vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "en4HCOG3TKfM",
        "colab_type": "code",
        "outputId": "aced1c41-6844-4c96-a437-af06d752a788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "v.itos[100:110]"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bad',\n",
              " 'people',\n",
              " 'will',\n",
              " 'other',\n",
              " 'also',\n",
              " 'into',\n",
              " 'first',\n",
              " 'because',\n",
              " 'great',\n",
              " 'how']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wss_v0OgTKfN",
        "colab_type": "code",
        "outputId": "6e0116ad-47d3-4588-a8c6-8cd04c7a954b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%%time\n",
        "val_term_doc = get_term_doc_matrix(reviews_full.valid.x, len(reviews_full.vocab.itos))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.08 s, sys: 173 ms, total: 5.25 s\n",
            "Wall time: 5.11 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wZ3XtE7fTKfP",
        "colab_type": "code",
        "outputId": "a2f4d901-0d7e-4378-e9c5-21faed867df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%%time\n",
        "trn_term_doc = get_term_doc_matrix(reviews_full.train.x, len(reviews_full.vocab.itos))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.55 s, sys: 189 ms, total: 5.74 s\n",
            "Wall time: 5.49 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLhZ4q3BTKfQ",
        "colab_type": "text"
      },
      "source": [
        "### Save data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5lW1GxOTKfQ",
        "colab_type": "text"
      },
      "source": [
        "That was slow.  Let's save our matrices for faster loading next time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hHLumETTKfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scipy.sparse.save_npz(\"trn_term_doc.npz\", trn_term_doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-_PAGzCTKfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scipy.sparse.save_npz(\"val_term_doc.npz\", val_term_doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhR1TG4UTKfS",
        "colab_type": "text"
      },
      "source": [
        "When storing data like this, always make sure it's included in your .gitignore file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrAGwvzgTKfT",
        "colab_type": "text"
      },
      "source": [
        "In the future, we'll just be able to load our data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkkhm2oTTKfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_term_doc = scipy.sparse.load_npz(\"trn_term_doc.npz\")\n",
        "val_term_doc = scipy.sparse.load_npz(\"val_term_doc.npz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QBTFtSJTKfY",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes on full dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfmYutqnTKfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=trn_term_doc\n",
        "y=reviews_full.train.y\n",
        "\n",
        "val_y = reviews_full.valid.y.items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx2BvQ3DTKfZ",
        "colab_type": "code",
        "outputId": "47122450-3313-4ad3-a12f-2a331a7304d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "x"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<25000x38456 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 3716267 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeTTrPnHTKfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive = y.c2i['pos']\n",
        "negative = y.c2i['neg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgtJGMtATKfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))\n",
        "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R13Dis_LTKfd",
        "colab_type": "code",
        "outputId": "dd0e8fc8-3177-445a-d748-fdb3000dedc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "p1[:20]"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 28449,      0,  12500,      0,      0, 342619,  20464,   1338,      7, 173122, 138001, 143763,  89570,  83404,\n",
              "        76828,  66715,  58510,  47896,  50177,  40451], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q67WAJ32TKfe",
        "colab_type": "text"
      },
      "source": [
        "### Data exploration: negative to positive ratios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3bGbz7vTKfe",
        "colab_type": "text"
      },
      "source": [
        "I was curious about the ratio of times a given word appears in negative reviews to times it occurs in positive reviews.  Bigger ratios (> 1) mean the word is indicative of a negative review, and smaller ratios (< 1) mean it is indicative of a positive review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuAna0JPTKff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neg_pos_given_word(word):\n",
        "    print(p0[v.stoi[word]]/p1[v.stoi[word]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kAeoCw9TKfj",
        "colab_type": "code",
        "outputId": "e8f021e1-80d0-4460-bbeb-ef4998193b9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "neg_pos_given_word('hated')"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.051546391752577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKdz7MSlTKfm",
        "colab_type": "code",
        "outputId": "f090adc6-f5ed-4a5f-fef2-4899b5959a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "neg_pos_given_word('liked')"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6424702058504875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZriiNN77TKfp",
        "colab_type": "code",
        "outputId": "2ce0d58f-7d4f-4825-8139-0f8e3e318d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "neg_pos_given_word('loved')"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3139963167587477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO9U75sOTKfr",
        "colab_type": "code",
        "outputId": "82516e21-f580-4d8e-86c6-26a8497d0a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "neg_pos_given_word('best')"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.48538961038961037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5HUt3RtTKft",
        "colab_type": "code",
        "outputId": "60a3c305-90f2-443a-9d03-aeffdb8bc9fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "neg_pos_given_word('worst')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.837301587301587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2egG2rYTKfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
        "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2YXpSScTKfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.log(pr1/pr0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IIr8NqGTKfy",
        "colab_type": "code",
        "outputId": "d5e76669-3650-4c74-8b1b-8dd3ab545aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "r[v.stoi['hated']]"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.7133498878774648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOfXDEXMTKfz",
        "colab_type": "code",
        "outputId": "5c536f5e-8334-41e5-e515-a60c03bda82c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "r[v.stoi['loved']]"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1563661500586044"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2DcWuwvTKf2",
        "colab_type": "code",
        "outputId": "13bb81cd-5285-4f14-cb62-0ea4a2da22b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "r[v.stoi['worst']]"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2.2826243504315076"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXm2TMxKTKf4",
        "colab_type": "code",
        "outputId": "1f2063ee-ccb5-42c9-a73b-70f1ffc717d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "r[v.stoi['best']]"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7225576052173609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLJ2YuvqTKf6",
        "colab_type": "text"
      },
      "source": [
        "### Back to Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yeox4LsjTKf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative = y.c2i['neg']\n",
        "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKZ0gZG1TKf9",
        "colab_type": "text"
      },
      "source": [
        "Since we have equal numbers of positive and negative reviews in this data set, b is 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6DzJs9FTKf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
        "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS5QAZ0UTKf-",
        "colab_type": "code",
        "outputId": "bf3ea35b-2a37-403d-9ffd-aa9f75370daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "b = np.log((y.items==positive).mean() / (y.items==negative).mean()); b"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrxJZzCwTKf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = (val_term_doc @ r + b) > 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgyhYzp1TKgA",
        "colab_type": "text"
      },
      "source": [
        "Our accuracy is 80% for the full data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AolC2_nATKgA",
        "colab_type": "code",
        "outputId": "c9179b2b-346a-4487-99dc-bca391a5a4ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(preds == val_y).mean()"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8084"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slkKHSOBTKgC",
        "colab_type": "text"
      },
      "source": [
        "### Binarized Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cipnoqdYTKgD",
        "colab_type": "text"
      },
      "source": [
        "Maybe it only matters whether a word is in the review or not (not the frequency of the word):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW10AuulTKgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=trn_term_doc.sign()\n",
        "y=reviews_full.train.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IqRp3NqTKgE",
        "colab_type": "code",
        "outputId": "c402e8e7-895d-407e-99d9-a960afcdcaa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "x.todense()[:10,:10]"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[1, 0, 1, 0, ..., 1, 1, 0, 1],\n",
              "        [1, 0, 1, 0, ..., 0, 0, 0, 1],\n",
              "        [1, 0, 1, 0, ..., 1, 0, 0, 1],\n",
              "        [1, 0, 1, 0, ..., 0, 0, 0, 1],\n",
              "        ...,\n",
              "        [1, 0, 1, 0, ..., 1, 0, 0, 1],\n",
              "        [1, 0, 1, 0, ..., 1, 0, 0, 1],\n",
              "        [0, 0, 1, 0, ..., 0, 0, 0, 1],\n",
              "        [1, 0, 1, 0, ..., 1, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F75gZY_CTKgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative = y.c2i['neg']\n",
        "positive = y.c2i['pos']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2S-PNAgTKgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))\n",
        "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFcBKE3fTKgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
        "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxSjGp17TKgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.log(pr1/pr0)\n",
        "b = np.log((y.items==positive).mean() / (y.items==negative).mean())\n",
        "\n",
        "preds = (val_term_doc.sign() @ r + b) > 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDeXPtjKTKgK",
        "colab_type": "code",
        "outputId": "40b24b83-10ab-4001-a092-236311dbf8de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(preds==val_y).mean()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.82924"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbDiLA4pTKgM",
        "colab_type": "text"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83RgBPFDTKgM",
        "colab_type": "text"
      },
      "source": [
        "Here is how we can fit logistic regression where the features are the unigrams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jka5U8ufTKgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRN3jMmMTKgN",
        "colab_type": "code",
        "outputId": "ac8c8977-cef1-4c9c-bd6f-a72c4f354561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(x, y.items.astype(int))\n",
        "preds = m.predict(val_term_doc)\n",
        "(preds==val_y).mean()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7704"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-px8cW3RTKgP",
        "colab_type": "text"
      },
      "source": [
        "And the binarized version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlcAFYJjTKgP",
        "colab_type": "code",
        "outputId": "78dd3fc2-cd43-4181-8127-d8e8d5760820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(trn_term_doc.sign(), y.items.astype(int))\n",
        "preds = m.predict(val_term_doc.sign())\n",
        "(preds==val_y).mean()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8854"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V73cj_7rTKgR",
        "colab_type": "text"
      },
      "source": [
        "# Trigram with NB features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwwnbeUvTKgR",
        "colab_type": "text"
      },
      "source": [
        "Our next model is a version of logistic regression with Naive Bayes features described [here](https://www.aclweb.org/anthology/P12-2018). For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yt8Dy6kTKgR",
        "colab_type": "text"
      },
      "source": [
        "### ngrams on full dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrQU7J_pTKgS",
        "colab_type": "text"
      },
      "source": [
        "An n-gram is a contiguous sequence of n items (where the items can be characters, syllables, or words).  A 1-gram is a unigram, a 2-gram is a bigram, and a 3-gram is a trigram.\n",
        "\n",
        "Here, we are referring to sequences of words. So examples of bigrams include \"the dog\", \"said that\", and \"can't you\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_vsuTX1TKgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = untar_data(URLs.IMDB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvb30lrqTKgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_full = (TextList.from_folder(path)\n",
        "             #grab all the text files in path\n",
        "             .split_by_folder(valid='test')\n",
        "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
        "             .label_from_folder(classes=['neg', 'pos']))\n",
        "             #label them all with their folders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG4nf9AaTKgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "01cedf53-eba3-4f31-e35f-8154466d4e26"
      },
      "source": [
        "v = reviews_full.vocab.itos\n",
        "vocab_len = len(v)\n",
        "vocab_len"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAbp5aDKTKgb",
        "colab_type": "text"
      },
      "source": [
        "## Our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_LuxCmjTKgb",
        "colab_type": "text"
      },
      "source": [
        "### Create train matrix\n",
        "\n",
        "use CountVectorizer because the for loop takes forever"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oThRGFdWTKgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2bf0dd80-1ca0-4c61-f892-1d5985a1b613"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "veczr = CountVectorizer(ngram_range=(1,3), preprocessor=noop, tokenizer=noop, max_features=800000)\n",
        "\n",
        "docs = reviews_full.train.x\n",
        "\n",
        "train_words = [[docs.vocab.itos[o] for o in doc.data] for doc in reviews_full.train.x]\n",
        "valid_words = [[docs.vocab.itos[o] for o in doc.data] for doc in reviews_full.valid.x]\n",
        "\n",
        "%time train_ngram_doc = veczr.fit_transform(train_words)"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 4s, sys: 2.11 s, total: 1min 6s\n",
            "Wall time: 1min 6s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L4XaaHtTKgg",
        "colab_type": "code",
        "outputId": "277e8f4f-5b16-4b9f-ffc8-f7d19c7fee00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "train_ngram_doc"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<25000x800000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 13437390 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9CkCRPMTKg9",
        "colab_type": "text"
      },
      "source": [
        "### Save data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waSCIpldTKg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scipy.sparse.save_npz(\"train_ngram_matrix.npz\", train_ngram_doc_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1R89Mg1TKg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scipy.sparse.save_npz(\"valid_ngram_matrix.npz\", valid_ngram_doc_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrWq_vHETKg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('itongram.pickle', 'wb') as handle:\n",
        "    pickle.dump(itongram, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "with open('ngramtoi.pickle', 'wb') as handle:\n",
        "    pickle.dump(itongram, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "N_HVy5RJTKhA",
        "colab_type": "text"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "Vkf7Fq8ZTKhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ngram_doc_matrix = scipy.sparse.load_npz(\"train_ngram_matrix.npz\")\n",
        "valid_ngram_doc_matrix = scipy.sparse.load_npz(\"valid_ngram_matrix.npz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "uzIhXVP0TKhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('itongram.pickle', 'rb') as handle:\n",
        "    b = pickle.load(handle)\n",
        "    \n",
        "with open('ngramtoi.pickle', 'rb') as handle:\n",
        "    b = pickle.load(handle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeHqrhLwTKhC",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23Aj5YcyTKhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=train_ngram_doc\n",
        "y=reviews_full.train.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teeHLi2dTKhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive = y.c2i['pos']\n",
        "negative = y.c2i['neg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXaAikmSTKhE",
        "colab_type": "code",
        "outputId": "2740dd53-d0f5-421e-f7a3-1bcbfc488ba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "x"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<25000x800000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 13437390 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU-zc0uOTKhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = (y.items == positive)\n",
        "neg = (y.items == negative)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnOGFVH9TKhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_labels = [o == positive for o in reviews_full.valid.y.items]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IyPGG1DTKhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p0 = np.squeeze(np.array(x[neg].sum(0)))\n",
        "p1 = np.squeeze(np.array(x[pos].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u88DWq9VTKhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
        "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q34MOFXITKhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.log(pr1/pr0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BogNnFZTKhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = np.log((y.items==positive).mean() / (y.items==negative).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZgpSof0TKhO",
        "colab_type": "code",
        "outputId": "59cf4e62-e64e-4dd7-81a3-b7a982489972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "b"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymcGqizATKhP",
        "colab_type": "code",
        "outputId": "a4530355-9c54-41a9-e6bf-e59c0d7e776c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(y.items==positive).mean(), (y.items==negative).mean()"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5, 0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VV7PKynD7ZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_ngram_doc = veczr.fit_transform(valid_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToumxZ6bTKhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_preds = valid_ngram_doc @ r.T + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFP3kE_yTKhR",
        "colab_type": "code",
        "outputId": "48dcfcde-74bf-4c66-ea2a-3b05cd4411c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pre_preds"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30.892863, 24.513466, 39.551624,  2.803927, ..., 35.659341, -9.579987, 60.109384,  1.840509])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9VsxOsqTKhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = pre_preds.T>0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzTWZJubTKhT",
        "colab_type": "code",
        "outputId": "c39cdf30-f02a-4421-8b32-b72678760ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "preds[:10]"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOYp5CCQTKhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_labels = [o == positive for o in reviews_full.valid.y.items]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYlV0tgtTKhU",
        "colab_type": "code",
        "outputId": "e1b0e293-2b34-4fa6-f0c6-6de57ee87589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(preds == valid_labels).mean()"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.49604"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx1a6_dHTKhV",
        "colab_type": "text"
      },
      "source": [
        "### Binarized Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25o2Od2JTKhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_x_ngram_sgn = train_ngram_doc.sign()\n",
        "val_x_ngram_sgn = valid_ngram_doc.sign()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcFaY8PETKhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = trn_x_ngram_sgn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvSFcgWATKhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p0 = np.squeeze(np.array(x[neg].sum(0)))\n",
        "p1 = np.squeeze(np.array(x[pos].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mSLlfOrTKhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
        "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxMXYoXvTKhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.log(pr1/pr0)\n",
        "b = np.log((y.items==positive).mean() / (y.items==negative).mean())\n",
        "\n",
        "pre_preds = val_x_ngram_sgn @ r.T + b\n",
        "preds = pre_preds.T>0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8r0N3uMTKhZ",
        "colab_type": "code",
        "outputId": "15b02e76-ae9b-4da1-b951-dcd2654bf54b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(preds==valid_labels).mean()"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.51236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_SN6lycTKha",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFtmbd2sTKha",
        "colab_type": "text"
      },
      "source": [
        "Here we fit regularized logistic regression where the features are the trigrams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSGvx6sUTKha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMIcOk4LTKhb",
        "colab_type": "text"
      },
      "source": [
        "### use CountVectorizer to compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSt4M2QYTKhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC9i5zVCTKhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "veczr = CountVectorizer(ngram_range=(1,3), preprocessor=noop, tokenizer=noop, max_features=800000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfH9sg3ITKhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs = reviews_full.train.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx4kk4bFTKhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_words = [[docs.vocab.itos[o] for o in doc.data] for doc in reviews_full.train.x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GRN8UWzTKhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_words = [[docs.vocab.itos[o] for o in doc.data] for doc in reviews_full.valid.x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwcLmM6ZTKhe",
        "colab_type": "code",
        "outputId": "1da6c659-24b9-49a6-f5f5-7ae98d048f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%%time\n",
        "train_ngram_doc = veczr.fit_transform(train_words)"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 4s, sys: 1.29 s, total: 1min 5s\n",
            "Wall time: 1min 5s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MzR56Ex0TKhf",
        "colab_type": "code",
        "outputId": "7cf47eb0-c66a-459e-ff6e-9394ca4cb1e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "train_ngram_doc"
      ],
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<25000x800000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 13437390 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvQM8qHNTKhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_ngram_doc = veczr.transform(valid_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tuvsbe_fTKhj",
        "colab_type": "code",
        "outputId": "025de5d4-9ed3-4835-9a7a-d9649c9c959c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "val_ngram_doc"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<25000x800000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 12503504 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAUkJf8RTKhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = veczr.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyNZi4hgTKhk",
        "colab_type": "code",
        "outputId": "fe65aa68-d311-45d4-fbce-a48be1459e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab[200000:200005]"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['common and', 'common as', 'common besides', 'common but', 'common cold']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpXMFUobTKhm",
        "colab_type": "text"
      },
      "source": [
        "#### Binarized Naive Bayes, using ngrams from CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4lmupgFTKhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=reviews_full.train.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkJxKdjGTKhn",
        "colab_type": "text"
      },
      "source": [
        "C is the inverse of regularization strength; smaller values specify stronger regularization.  Regularized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isRw0xivTKhn",
        "colab_type": "code",
        "outputId": "3df016ff-8a01-427c-ad1f-fb24d431ca9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(train_ngram_doc.sign(), y.items);\n",
        "\n",
        "preds = m.predict(val_ngram_doc.sign())\n",
        "(preds.T==valid_labels).mean()"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.903"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHaEeivnTKho",
        "colab_type": "text"
      },
      "source": [
        "Not binarized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFabYNquTKho",
        "colab_type": "code",
        "outputId": "3d8534af-ec06-427f-ed57-f4e0eb3d7c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(train_ngram_doc, y.items);\n",
        "\n",
        "preds = m.predict(val_ngram_doc)\n",
        "(preds.T==valid_labels).mean()"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.89704"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBH6q6kqTKht",
        "colab_type": "text"
      },
      "source": [
        "### Log-count ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCVE0lVmTKht",
        "colab_type": "text"
      },
      "source": [
        "Here is the $\\text{log-count ratio}$ `r`.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdNcJAVpTKhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=train_ngram_doc.sign()\n",
        "val_x=val_ngram_doc.sign()\n",
        "y=reviews_full.train.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3nUVOoVTKhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = (y.items == positive)\n",
        "neg = (y.items == negative)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPuq3sarTKhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p0 = np.squeeze(np.array(x[neg].sum(0)))\n",
        "p1 = np.squeeze(np.array(x[pos].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow49EnHbTKhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
        "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjddoDdTTKh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.log(pr1/pr0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BG9NkUqTKh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = np.log((y.items==positive).mean() / (y.items==negative).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSJ6W8q6TKh3",
        "colab_type": "code",
        "outputId": "56bf8c04-5d91-4ea6-949e-757a6bcc31c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.exp(r)"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.952565, 1.8     , 1.8     , 1.5     , ..., 4.      , 0.5     , 0.5     , 0.5     ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox4n8lZiTKh3",
        "colab_type": "text"
      },
      "source": [
        "Here we fit regularized logistic regression where the features are the trigrams' log-count ratios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TneMqxOzTKh4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "46f0ee7f-9716-40aa-aa74-7c51e6a63499"
      },
      "source": [
        "x_nb = x.multiply(r)\n",
        "m = LogisticRegression(dual=True, C=0.1)\n",
        "m.fit(x_nb, y.items);\n",
        "\n",
        "val_x_nb = val_x.multiply(r)\n",
        "preds = m.predict(val_x_nb)\n",
        "(preds.T==valid_labels).mean()"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.91832"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99v_UnJ2TKh4",
        "colab_type": "text"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRC8xRS5TKh5",
        "colab_type": "text"
      },
      "source": [
        "* Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. Sida Wang and Christopher D. Manning [pdf](https://www.aclweb.org/anthology/P12-2018)"
      ]
    }
  ]
}